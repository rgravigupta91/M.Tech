{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Vjykoo3szYVB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716458535142,"user_tz":-330,"elapsed":37601,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}},"outputId":"234c08b0-884c-4fb9-a68d-3dc76a3211eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObwAnCv7m4_Z"},"outputs":[],"source":["#!wget https://www.dropbox.com/s/rbajpdlh7efkdo1/male_female_face_images.zip\n","!unzip -q /content/drive/MyDrive/GAN/male_female_face_images.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BX08hMBtnDeW","outputId":"4ccaa34c-ce07-4a50-8ec1-5d3d30bb6f2d","executionInfo":{"status":"ok","timestamp":1716458615582,"user_tz":-330,"elapsed":35771,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q --upgrade torch_snippets\n","from torch_snippets import *\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","from torchvision.utils import make_grid\n","from torch_snippets import *\n","from PIL import Image\n","import torchvision\n","from torchvision import transforms\n","import torchvision.utils as vutils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eXBqx39nH80"},"outputs":[],"source":["female_images = Glob('/content/females/*.jpg')\n","male_images = Glob('/content/males/*.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oq_QuchQnJrk"},"outputs":[],"source":["face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvKcMegYnLuf"},"outputs":[],"source":["!mkdir cropped_faces_female\n","!mkdir cropped_faces_male\n","for i in range(len(female_images)):\n","    img = read(female_images[i],1)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","    for (x,y,w,h) in faces:\n","        img2 = img[y:(y+h),x:(x+w),:]\n","    cv2.imwrite('cropped_faces_female/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n","for i in range(len(male_images)):\n","    img = read(male_images[i],1)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","    for (x,y,w,h) in faces:\n","        img2 = img[y:(y+h),x:(x+w),:]\n","    cv2.imwrite('cropped_faces_male/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))"]},{"cell_type":"code","source":["# Create directories to store cropped faces of females and males\n","!mkdir cropped_faces_female\n","!mkdir cropped_faces_male\n","\n","# Iterate over each female image\n","for i in range(len(female_images)):\n","    # Read the current female image\n","    img = read(female_images[i], 1)\n","\n","    # Convert the image to grayscale\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Detect faces in the grayscale image using the face cascade classifier\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","    # Iterate over each detected face\n","    for (x, y, w, h) in faces:\n","        # Crop the detected face region from the original image\n","        img2 = img[y:(y+h), x:(x+w), :]\n","\n","    # Write the cropped face image to the 'cropped_faces_female' directory\n","    cv2.imwrite('cropped_faces_female/' + str(i) + '.jpg', cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n","\n","# Iterate over each male image\n","for i in range(len(male_images)):\n","    # Read the current male image\n","    img = read(male_images[i], 1)\n","\n","    # Convert the image to grayscale\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Detect faces in the grayscale image using the face cascade classifier\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","    # Iterate over each detected face\n","    for (x, y, w, h) in faces:\n","        # Crop the detected face region from the original image\n","        img2 = img[y:(y+h), x:(x+w), :]\n","\n","    # Write the cropped face image to the 'cropped_faces_male' directory\n","    cv2.imwrite('cropped_faces_male/' + str(i) + '.jpg', cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n"],"metadata":{"id":"j1DP64AY4oCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4si0y8F-nNqC"},"outputs":[],"source":["transform=transforms.Compose([\n","                               transforms.Resize(64),\n","                               transforms.CenterCrop(64),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-_lQ0JSo7RO"},"outputs":[],"source":["class Faces(Dataset):\n","    def __init__(self, folders):\n","        super().__init__()\n","        self.folderfemale = folders[0]\n","        self.foldermale = folders[1]\n","        self.images=sorted(Glob(self.folderfemale))+sorted(Glob(self.foldermale))\n","    def __len__(self):\n","        return len(self.images)\n","    def __getitem__(self, ix):\n","        image_path = self.images[ix]\n","        image = Image.open(image_path)\n","        image = transform(image)\n","        gender = np.where('female' in str(image_path),1,0)\n","        return image, torch.tensor(gender).long()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-3zgclepGlv","outputId":"5a7c0014-93ef-4210-9bce-8c461ab8629c","executionInfo":{"status":"ok","timestamp":1716458956119,"user_tz":-330,"elapsed":809,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["ds = Faces(folders=['cropped_faces_female','cropped_faces_male'])\n","dataloader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C184MaaTpQcr"},"outputs":[],"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cALu7xyMpSjw"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, emb_size=32):\n","        super(Discriminator, self).__init__()\n","        self.emb_size = 32\n","        self.label_embeddings = nn.Embedding(2, self.emb_size)\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3,64,4,2,1,bias=False),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(64,64*2,4,2,1,bias=False),\n","            nn.BatchNorm2d(64*2),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n","            nn.BatchNorm2d(64*4),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n","            nn.BatchNorm2d(64*8),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(64*8,64,4,2,1,bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Flatten()\n","        )\n","        self.model2 = nn.Sequential(\n","            nn.Linear(288,100),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Linear(100,1),\n","            nn.Sigmoid()\n","        )\n","        self.apply(weights_init)\n","    def forward(self, input, labels):\n","        x = self.model(input)\n","        y = self.label_embeddings(labels)\n","        input = torch.cat([x, y], 1)\n","        final_output = self.model2(input)\n","        return final_output"]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, emb_size=32):\n","        super(Discriminator, self).__init__()\n","\n","        # Initialize the Discriminator class with an embedding size of 32\n","        self.emb_size = 32\n","\n","        # Define an embedding layer to embed binary labels (0 or 1) into an embedding space\n","        self.label_embeddings = nn.Embedding(2, self.emb_size)\n","\n","        # Define the convolutional layers for feature extraction\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1, bias=False),   # First convolutional layer\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Conv2d(64, 64*2, 4, 2, 1, bias=False),  # Second convolutional layer\n","            nn.BatchNorm2d(64*2),                    # Apply batch normalization\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Conv2d(64*2, 64*4, 4, 2, 1, bias=False),  # Third convolutional layer\n","            nn.BatchNorm2d(64*4),                    # Apply batch normalization\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Conv2d(64*4, 64*8, 4, 2, 1, bias=False),  # Fourth convolutional layer\n","            nn.BatchNorm2d(64*8),                    # Apply batch normalization\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Conv2d(64*8, 64, 4, 2, 1, bias=False),  # Fifth convolutional layer\n","            nn.BatchNorm2d(64),                      # Apply batch normalization\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Flatten()                             # Flatten the output tensor\n","        )\n","\n","        # Define the fully connected layers for classification\n","        self.model2 = nn.Sequential(\n","            nn.Linear(288, 100),                     # Fully connected layer with input size 288 and output size 100\n","            nn.LeakyReLU(0.2, inplace=True),        # Apply LeakyReLU activation function\n","            nn.Linear(100, 1),                       # Fully connected layer with input size 100 and output size 1\n","            nn.Sigmoid()                             # Apply Sigmoid activation function to output probability\n","        )\n","\n","        # Initialize the weights of the discriminator using a custom initialization function\n","        self.apply(weights_init)\n","\n","    def forward(self, input, labels):\n","        # Forward pass through the convolutional layers\n","        x = self.model(input)\n","\n","        # Embed the binary labels into the embedding space\n","        y = self.label_embeddings(labels)\n","\n","        # Concatenate the features from convolutional layers and the label embeddings\n","        input = torch.cat([x, y], 1) #concatenates tensors x and y along their second dimension, combining their features horizontally.\n","\n","        # Forward pass through the fully connected layers for final classification\n","        final_output = self.model2(input)\n","\n","        # Return the final output\n","        return final_output\n"],"metadata":{"id":"-lBWD2q-3q6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWkFJ6eBpU3c","outputId":"abc77394-224a-4216-9292-ceff453e70ac","executionInfo":{"status":"ok","timestamp":1716458975797,"user_tz":-330,"elapsed":7351,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_summary\n","  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n","Installing collected packages: torch_summary\n","Successfully installed torch_summary-1.4.5\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Sequential: 1-1                        [-1, 256]                 --\n","|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          3,072\n","|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n","|    └─Conv2d: 2-3                       [-1, 128, 16, 16]         131,072\n","|    └─BatchNorm2d: 2-4                  [-1, 128, 16, 16]         256\n","|    └─LeakyReLU: 2-5                    [-1, 128, 16, 16]         --\n","|    └─Conv2d: 2-6                       [-1, 256, 8, 8]           524,288\n","|    └─BatchNorm2d: 2-7                  [-1, 256, 8, 8]           512\n","|    └─LeakyReLU: 2-8                    [-1, 256, 8, 8]           --\n","|    └─Conv2d: 2-9                       [-1, 512, 4, 4]           2,097,152\n","|    └─BatchNorm2d: 2-10                 [-1, 512, 4, 4]           1,024\n","|    └─LeakyReLU: 2-11                   [-1, 512, 4, 4]           --\n","|    └─Conv2d: 2-12                      [-1, 64, 2, 2]            524,288\n","|    └─BatchNorm2d: 2-13                 [-1, 64, 2, 2]            128\n","|    └─LeakyReLU: 2-14                   [-1, 64, 2, 2]            --\n","|    └─Flatten: 2-15                     [-1, 256]                 --\n","├─Embedding: 1-2                         [-1, 32]                  64\n","├─Sequential: 1-3                        [-1, 1]                   --\n","|    └─Linear: 2-16                      [-1, 100]                 28,900\n","|    └─LeakyReLU: 2-17                   [-1, 100]                 --\n","|    └─Linear: 2-18                      [-1, 1]                   101\n","|    └─Sigmoid: 2-19                     [-1, 1]                   --\n","==========================================================================================\n","Total params: 3,310,857\n","Trainable params: 3,310,857\n","Non-trainable params: 0\n","Total mult-adds (M): 109.25\n","==========================================================================================\n","Input size (MB): 1.50\n","Forward/backward pass size (MB): 1.38\n","Params size (MB): 12.63\n","Estimated Total Size (MB): 15.51\n","==========================================================================================\n"]}],"source":["!pip install torch_summary\n","from torchsummary import summary\n","discriminator = Discriminator().to(device)\n","summary(discriminator,torch.zeros(32,3,64,64).to(device), torch.zeros(32).long().to(device));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vJXr2BEpZAZ"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, emb_size=32):\n","        super(Generator,self).__init__()\n","        self.emb_size = emb_size\n","        self.label_embeddings = nn.Embedding(2, self.emb_size)\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(100+self.emb_size,64*8,4,1,0,bias=False),\n","            nn.BatchNorm2d(64*8),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n","            nn.BatchNorm2d(64*4),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64*4,64*2,4,2,1,bias=False),\n","            nn.BatchNorm2d(64*2),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64*2,64,4,2,1,bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64,3,4,2,1,bias=False),\n","            nn.Tanh()\n","        )\n","        self.apply(weights_init)\n","    def forward(self,input_noise,labels):\n","        label_embeddings = self.label_embeddings(labels).view(len(labels), self.emb_size, 1, 1)\n","        input = torch.cat([input_noise, label_embeddings], 1)\n","        return self.model(input)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class Generator(nn.Module):\n","    def __init__(self, emb_size=32):\n","        # Initialize the Generator class and set up the model layers\n","        super(Generator, self).__init__()\n","        self.emb_size = emb_size\n","\n","        # Define an embedding layer to convert labels into embeddings\n","        # 2 is the number of classes, emb_size is the size of the embedding vector for each class\n","        self.label_embeddings = nn.Embedding(2, self.emb_size)\n","\n","        # Define the generator model using a sequential container\n","        self.model = nn.Sequential(\n","            # First transposed convolutional layer\n","            # Input: Noise vector + label embedding, Output: Feature maps\n","            nn.ConvTranspose2d(100 + self.emb_size, 64 * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(64 * 8),  # Batch normalization layer\n","            nn.ReLU(True),  # ReLU activation function\n","\n","            # Second transposed convolutional layer\n","            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(64 * 4),  # Batch normalization layer\n","            nn.ReLU(True),  # ReLU activation function\n","\n","            # Third transposed convolutional layer\n","            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(64 * 2),  # Batch normalization layer\n","            nn.ReLU(True),  # ReLU activation function\n","\n","            # Fourth transposed convolutional layer\n","            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(64),  # Batch normalization layer\n","            nn.ReLU(True),  # ReLU activation function\n","\n","            # Output layer: Transposed convolutional layer\n","            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n","            nn.Tanh()  # Tanh activation function to output values in the range [-1, 1]\n","        )\n","\n","        # Apply weight initialization to the model\n","        self.apply(weights_init)\n","\n","    def forward(self, input_noise, labels):\n","        # Forward pass through the generator\n","\n","        # Convert labels to embeddings and reshape to match the dimensions of input_noise\n","        label_embeddings = self.label_embeddings(labels).view(len(labels), self.emb_size, 1, 1)\n","\n","        # Concatenate the input noise and the label embeddings along the channel dimension\n","        input = torch.cat([input_noise, label_embeddings], 1)\n","\n","        # Pass the concatenated tensor through the generator model to produce the output\n","        return self.model(input)\n"],"metadata":{"id":"ARQQtt4AP9E6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIJXW1SxphF7","outputId":"6031b0eb-4151-4fbd-a05c-517470e3df0a","executionInfo":{"status":"ok","timestamp":1716458984622,"user_tz":-330,"elapsed":523,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Embedding: 1-1                         [-1, 32]                  64\n","├─Sequential: 1-2                        [-1, 3, 64, 64]           --\n","|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           1,081,344\n","|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n","|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n","|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n","|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n","|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n","|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n","|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n","|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n","|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n","|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n","|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n","|    └─ConvTranspose2d: 2-13             [-1, 3, 64, 64]           3,072\n","|    └─Tanh: 2-14                        [-1, 3, 64, 64]           --\n","==========================================================================================\n","Total params: 3,838,912\n","Trainable params: 3,838,912\n","Non-trainable params: 0\n","Total mult-adds (M): 436.38\n","==========================================================================================\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.97\n","Params size (MB): 14.64\n","Estimated Total Size (MB): 16.63\n","==========================================================================================\n"]}],"source":["generator = Generator().to(device)\n","summary(generator,torch.zeros(32,100,1,1).to(device), torch.zeros(32).long().to(device));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ry14Ju7LpjCL"},"outputs":[],"source":["def noise(size):\n","    n = torch.randn(size, 100, 1, 1, device=device)\n","    return n.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQLdvHvQpkyV"},"outputs":[],"source":["def discriminator_train_step(real_data, real_labels, fake_data, fake_labels):\n","    d_optimizer.zero_grad()\n","    prediction_real = discriminator(real_data, real_labels)\n","    error_real = loss(prediction_real, torch.ones(len(real_data), 1).to(device))\n","    error_real.backward()\n","    prediction_fake = discriminator(fake_data, fake_labels)\n","    error_fake = loss(prediction_fake, torch.zeros(len(fake_data), 1).to(device))\n","    error_fake.backward()\n","    d_optimizer.step()\n","    return error_real + error_fake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOO2Qr_KprEE"},"outputs":[],"source":["def generator_train_step(fake_data, fake_labels):\n","    g_optimizer.zero_grad()\n","    prediction = discriminator(fake_data, fake_labels)\n","    error = loss(prediction, torch.ones(len(fake_data), 1).to(device))\n","    error.backward()\n","    g_optimizer.step()\n","    return error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBkGVuKKptJQ"},"outputs":[],"source":["discriminator = Discriminator().to(device)\n","generator = Generator().to(device)\n","loss = nn.BCELoss()\n","d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n","fixed_fake_labels = torch.LongTensor([0]*(len(fixed_noise)//2) + [1]*(len(fixed_noise)//2)).to(device)\n","loss = nn.BCELoss()\n","n_epochs = 2 #25\n","img_list = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"13LqO0B4vTXBeMwWpyD6ob-uaC63bn_WR"},"id":"DszoT7j5pvzS","outputId":"0cb37a7b-2bce-4202-9baf-19e30f12eed1","executionInfo":{"status":"ok","timestamp":1716466265292,"user_tz":-330,"elapsed":5145339,"user":{"displayName":"Milan Joshi","userId":"13016873222319412216"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["log = Report(n_epochs)\n","for epoch in range(n_epochs):\n","    N = len(dataloader)\n","    for bx, (images, labels) in enumerate(dataloader):\n","        real_data, real_labels = images.to(device), labels.to(device)\n","        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n","        fake_data = generator(noise(len(real_data)), fake_labels)\n","        fake_data = fake_data.detach()\n","        d_loss = discriminator_train_step(real_data, real_labels, fake_data, fake_labels)\n","        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n","        fake_data = generator(noise(len(real_data)), fake_labels).to(device)\n","        g_loss = generator_train_step(fake_data, fake_labels)\n","        pos = epoch + (1+bx)/N\n","        log.record(pos, d_loss=d_loss.detach(), g_loss=g_loss.detach(), end='\\r')\n","    log.report_avgs(epoch+1)\n","    with torch.no_grad():\n","        fake = generator(fixed_noise, fixed_fake_labels).detach().cpu()\n","        imgs = vutils.make_grid(fake, padding=2, normalize=True).permute(1,2,0)\n","        img_list.append(imgs)\n","        show(imgs, sz=10)\n","# 42.65 minits for one epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S7UOR3ep-_f"},"outputs":[],"source":["# Initialize a log to track the progress of the training over n_epochs\n","log = Report(n_epochs)\n","\n","# Start the training loop for the specified number of epochs\n","for epoch in range(n_epochs):\n","    # Get the total number of batches in the dataloader\n","    N = len(dataloader)\n","\n","    # Loop over each batch of data\n","    for bx, (images, labels) in enumerate(dataloader):\n","        # Move the real images and labels to the specified device (CPU or GPU)\n","        real_data, real_labels = images.to(device), labels.to(device)\n","\n","        # Generate random labels for the fake data\n","        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n","\n","        # Generate fake data using the generator with the random noise and fake labels\n","        fake_data = generator(noise(len(real_data)), fake_labels)\n","\n","        # Detach the fake data so that its gradients are not tracked during the discriminator's update\n","        fake_data = fake_data.detach()\n","\n","        # Train the discriminator with the real and fake data, and get the discriminator loss\n","        d_loss = discriminator_train_step(real_data, real_labels, fake_data, fake_labels)\n","\n","        # Generate new random labels for the fake data\n","        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n","\n","        # Generate fake data again using the generator with the new random noise and fake labels\n","        fake_data = generator(noise(len(real_data)), fake_labels).to(device)\n","\n","        # Train the generator to improve its performance in generating realistic data, and get the generator loss\n","        g_loss = generator_train_step(fake_data, fake_labels)\n","\n","        # Calculate the current position in terms of epochs and batches\n","        pos = epoch + (1 + bx) / N\n","\n","        # Record the discriminator and generator losses for the current position\n","        log.record(pos, d_loss=d_loss.detach(), g_loss=g_loss.detach(), end='\\r')\n","\n","    # Report the average losses for the current epoch\n","    log.report_avgs(epoch + 1)\n","\n","    # Generate and visualize the fake images using the fixed noise and fixed fake labels for monitoring progress\n","    with torch.no_grad():\n","        fake = generator(fixed_noise, fixed_fake_labels).detach().cpu()\n","        imgs = vutils.make_grid(fake, padding=2, normalize=True).permute(1, 2, 0)\n","        img_list.append(imgs)\n","        show(imgs, sz=10)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}