{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211ee077",
   "metadata": {},
   "source": [
    "# Step 1: Define the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f3e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "# ReLU activation and its derivative\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "# Loss function (Binary Cross-Entropy)\n",
    "def binary_cross_entropy_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / m\n",
    "\n",
    "def binary_cross_entropy_loss_derivative(y, y_hat):\n",
    "    return -(y / y_hat) + ((1 - y) / (1 - y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b404f",
   "metadata": {},
   "source": [
    "# Define Architecture\n",
    "\n",
    "\n",
    "# Neural Network Architecture Overview\n",
    "\n",
    "| Layer           | Number of Neurons |\n",
    "|------------------|-------------------|\n",
    "| Input Layer      | 2                |\n",
    "| Hidden Layer 1   | 3                |\n",
    "| Hidden Layer 2   | 2                |\n",
    "| Output Layer     | 1                |\n",
    "\n",
    "## Total Number of Parameters\n",
    "To compute the total parameters:\n",
    "- **Weights**:\n",
    "  - Layer 1: $ 2 \\times 3 $\n",
    "  - Layer 2: $ 3 \\times 2 $\n",
    "  - Layer 3: $ 2 \\times 1 $\n",
    "- **Biases**:\n",
    "  - Layer 1: $3$\n",
    "  - Layer 2: $ 2 $\n",
    "  - Layer 3: $ 1 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64130b",
   "metadata": {},
   "source": [
    "# Step 2: Initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5ad058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters (weights and biases)\n",
    "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    W1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden1_size))\n",
    "    W2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "    b2 = np.zeros((1, hidden2_size))\n",
    "    W3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "    b3 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a9fb1",
   "metadata": {},
   "source": [
    "# Step 3: Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
    "    # Layer 1\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    H1 = relu(Z1)  # Use relu or sigmoid here\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = np.dot(H1, W2) + b2\n",
    "    H2 = relu(Z2)  # Use relu or sigmoid here\n",
    "\n",
    "    # Layer 3 (Output layer)\n",
    "    Z3 = np.dot(H2, W3) + b3\n",
    "    y_hat = sigmoid(Z3)  # Use sigmoid for binary classification\n",
    "\n",
    "    cache = (Z1, H1, Z2, H2, Z3, y_hat)\n",
    "    return y_hat, cache   # Cache Stores called Memoization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9d468",
   "metadata": {},
   "source": [
    "# Step 4: Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd0a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, y, cache, W1, W2, W3):\n",
    "    Z1, H1, Z2, H2, Z3, y_hat = cache\n",
    "    m = y.shape[0]\n",
    "\n",
    "    # Output layer gradients\n",
    "    dL_dy_hat = binary_cross_entropy_loss_derivative(y, y_hat)  # dL/dy_hat\n",
    "    dy_hat_dZ3 = sigmoid_derivative(Z3)  # dy_hat/dZ3\n",
    "    dZ3 = dL_dy_hat * dy_hat_dZ3  # dL/dZ3\n",
    "    dW3 = np.dot(H2.T, dZ3) / m  # dL/dW3\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m  # dL/db3 Computes the average gradient of the bias over all examples in the batch\n",
    "\n",
    "    # Hidden layer 2 gradients\n",
    "    dZ3_dH2 = W3\n",
    "    dH2 = np.dot(dZ3, dZ3_dH2.T)  # dL/dH2\n",
    "    dH2_dZ2 = relu_derivative(Z2)  # dH2/dZ2\n",
    "    dZ2 = dH2 * dH2_dZ2  # dL/dZ2\n",
    "    dW2 = np.dot(H1.T, dZ2) / m  # dL/dW2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # dL/db2\n",
    "\n",
    "    # Hidden layer 1 gradients\n",
    "    dZ2_dH1 = W2\n",
    "    dH1 = np.dot(dZ2, dZ2_dH1.T)  # dL/dH1\n",
    "    dH1_dZ1 = relu_derivative(Z1)  # dH1/dZ1\n",
    "    dZ1 = dH1 * dH1_dZ1  # dL/dZ1\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # dL/dW1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # dL/db1\n",
    "\n",
    "    gradients = (dW1, db1, dW2, db2, dW3, db3)\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c5743",
   "metadata": {},
   "source": [
    "# Step 5: Update weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c659281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate):\n",
    "    dW1, db1, dW2, db2, dW3, db3 = gradients\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997a857",
   "metadata": {},
   "source": [
    "# Step 6: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62bd5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, input_size, hidden1_size, hidden2_size, output_size, learning_rate, num_epochs):\n",
    "    # Initialize parameters\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        y_hat, cache = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = binary_cross_entropy_loss(y, y_hat)\n",
    "\n",
    "        # Backward propagation\n",
    "        gradients = backward_propagation(X, y, cache, W1, W2, W3)\n",
    "\n",
    "        # Update parameters\n",
    "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5699bc",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4db34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.693147193959094\n",
      "Epoch 100, Loss: 0.6918868361872207\n",
      "Epoch 200, Loss: 0.6911227604659338\n",
      "Epoch 300, Loss: 0.6906593902962515\n",
      "Epoch 400, Loss: 0.690378271957255\n",
      "Epoch 500, Loss: 0.6902076564962265\n",
      "Epoch 600, Loss: 0.6901040706560907\n",
      "Epoch 700, Loss: 0.6900411613590189\n",
      "Epoch 800, Loss: 0.6900029457981876\n",
      "Epoch 900, Loss: 0.6899797260751569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0049661 , -0.00137855,  0.00647645,  0.01523046],\n",
       "        [-0.00234047, -0.002331  ,  0.01579848,  0.00767192],\n",
       "        [-0.00469609,  0.00543673, -0.00464249, -0.00465411]]),\n",
       " array([[ 4.28122513e-06,  1.69158433e-05, -6.86745155e-07,\n",
       "          3.70247739e-07]]),\n",
       " array([[ 0.00241785, -0.01913296, -0.01724918, -0.00562288],\n",
       "        [-0.01013082,  0.00314415, -0.00908024, -0.01412304],\n",
       "        [ 0.01466473, -0.00226448,  0.00067528, -0.01424748],\n",
       "        [-0.00543889,  0.00110547, -0.01150994,  0.00375698]]),\n",
       " array([[-6.90029424e-05,  9.85695080e-06,  0.00000000e+00,\n",
       "          0.00000000e+00]]),\n",
       " array([[-0.00602591],\n",
       "        [-0.00292356],\n",
       "        [-0.00601707],\n",
       "        [ 0.01852278]]),\n",
       " array([[0.14707431]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example inputs\n",
    "X = np.random.rand(100, 3)  # 100 examples, 3 features\n",
    "y = np.random.randint(0, 2, size=(100, 1))  # Binary labels (0 or 1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3\n",
    "hidden1_size = 4\n",
    "hidden2_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Train the model\n",
    "train(X, y, input_size, hidden1_size, hidden2_size, output_size, learning_rate, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8e846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6931, Accuracy = 0.4900\n",
      "Epoch 100: Loss = 0.6928, Accuracy = 0.5200\n",
      "Epoch 200: Loss = 0.6926, Accuracy = 0.5200\n",
      "Epoch 300: Loss = 0.6925, Accuracy = 0.5200\n",
      "Epoch 400: Loss = 0.6925, Accuracy = 0.5200\n",
      "Epoch 500: Loss = 0.6924, Accuracy = 0.5200\n",
      "Epoch 600: Loss = 0.6924, Accuracy = 0.5200\n",
      "Epoch 700: Loss = 0.6924, Accuracy = 0.5200\n",
      "Epoch 800: Loss = 0.6924, Accuracy = 0.5200\n",
      "Epoch 900: Loss = 0.6924, Accuracy = 0.5200\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00496819, -0.00137718,  0.00648758,  0.01522632],\n",
       "        [-0.00234026, -0.00233054,  0.01579971,  0.00767151],\n",
       "        [-0.00469466,  0.00544062, -0.0046459 , -0.00465302]]),\n",
       " array([[ 2.03017520e-06,  2.33134993e-05,  2.72688074e-07,\n",
       "         -1.13023681e-07]]),\n",
       " array([[ 0.00242039, -0.0191328 , -0.01724918, -0.00562288],\n",
       "        [-0.01013315,  0.00314249, -0.00908024, -0.01412304],\n",
       "        [ 0.01467303, -0.00225814,  0.00067528, -0.01424748],\n",
       "        [-0.00542515,  0.00110854, -0.01150994,  0.00375698]]),\n",
       " array([[ 1.63231662e-05, -7.30879115e-06,  0.00000000e+00,\n",
       "          0.00000000e+00]]),\n",
       " array([[-0.00603827],\n",
       "        [-0.002917  ],\n",
       "        [-0.00601707],\n",
       "        [ 0.01852278]]),\n",
       " array([[0.07347499]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize parameters (weights and biases)\n",
    "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    W1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden1_size))\n",
    "    W2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "    b2 = np.zeros((1, hidden2_size))\n",
    "    W3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "    b3 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = np.maximum(0, Z1)  # ReLU activation\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = np.maximum(0, Z2)  # ReLU activation\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = sigmoid(Z3)  # Sigmoid activation for output layer\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dZ3 = A3 - y\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA2 = np.dot(dZ3, W3.T)\n",
    "    dZ2 = dA2 * (Z2 > 0)  # Derivative of ReLU\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # Derivative of ReLU\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "# Compute binary cross-entropy loss\n",
    "def compute_loss(y, A3):\n",
    "    m = y.shape[0]\n",
    "    loss = -np.mean(y * np.log(A3 + 1e-8) + (1 - y) * np.log(1 - A3 + 1e-8))  # Adding small value for numerical stability\n",
    "    return loss\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_accuracy(y, A3):\n",
    "    predictions = A3 >= 0.5\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    return accuracy\n",
    "\n",
    "# Training function\n",
    "def train(X, y, input_size, hidden1_size, hidden2_size, output_size, learning_rate, num_epochs):\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, A3)\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = compute_accuracy(y, A3)\n",
    "\n",
    "        # Backward propagation\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n",
    "\n",
    "        # Update parameters\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W3 -= learning_rate * dW3\n",
    "        b3 -= learning_rate * db3\n",
    "\n",
    "        # Print loss and accuracy every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "# Example inputs\n",
    "X = np.random.rand(100, 3)  # 100 examples, 3 features\n",
    "y = np.random.randint(0, 2, size=(100, 1))  # Binary labels (0 or 1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3\n",
    "hidden1_size = 4\n",
    "hidden2_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Train the model\n",
    "train(X, y, input_size, hidden1_size, hidden2_size, output_size, learning_rate, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef68dc9",
   "metadata": {},
   "source": [
    "# Backwordpropogation with Memoization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab090170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(y, cache, W1, W2, W3):\n",
    "    # Retrieve cached values\n",
    "    X = cache[\"X\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    H1 = cache[\"H1\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    H2 = cache[\"H2\"]\n",
    "    Z3 = cache[\"Z3\"]\n",
    "    y_hat = cache[\"y_hat\"]\n",
    "\n",
    "    m = y.shape[0]  # Number of examples\n",
    "\n",
    "    # Gradients for Output Layer\n",
    "    dL_dy_hat = binary_cross_entropy_loss_derivative(y, y_hat)  # dL/dy_hat\n",
    "    dy_hat_dZ3 = sigmoid_derivative(Z3)                        # dy_hat/dZ3\n",
    "    dZ3 = dL_dy_hat * dy_hat_dZ3                               # dL/dZ3\n",
    "    dW3 = np.dot(H2.T, dZ3) / m                                # dL/dW3\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m               # dL/db3\n",
    "\n",
    "    # Gradients for Hidden Layer 2\n",
    "    dZ3_dH2 = W3\n",
    "    dH2 = np.dot(dZ3, dZ3_dH2.T)                               # dL/dH2\n",
    "    dH2_dZ2 = relu_derivative(Z2)                              # dH2/dZ2\n",
    "    dZ2 = dH2 * dH2_dZ2                                        # dL/dZ2\n",
    "    dW2 = np.dot(H1.T, dZ2) / m                                # dL/dW2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m               # dL/db2\n",
    "\n",
    "    # Gradients for Hidden Layer 1\n",
    "    dZ2_dH1 = W2\n",
    "    dH1 = np.dot(dZ2, dZ2_dH1.T)                               # dL/dH1\n",
    "    dH1_dZ1 = relu_derivative(Z1)                              # dH1/dZ1\n",
    "    dZ1 = dH1 * dH1_dZ1                                        # dL/dZ1\n",
    "    dW1 = np.dot(X.T, dZ1) / m                                 # dL/dW1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m               # dL/db1\n",
    "\n",
    "    # Store gradients in a dictionary\n",
    "    gradients = {\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1,\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2,\n",
    "        \"dW3\": dW3,\n",
    "        \"db3\": db3\n",
    "    }\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = backward_propagation(y, cache, W1, W2, W3)\n",
    "print(\"Gradients for W1:\", gradients[\"dW1\"])\n",
    "print(\"Gradients for b1:\", gradients[\"db1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate):\n",
    "    # Retrieve gradients\n",
    "    dW1, db1 = gradients[\"dW1\"], gradients[\"db1\"]\n",
    "    dW2, db2 = gradients[\"dW2\"], gradients[\"db2\"]\n",
    "    dW3, db3 = gradients[\"dW3\"], gradients[\"db3\"]\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
