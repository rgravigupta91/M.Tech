{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9754b-3faa-4632-8b9a-ef773978dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 'Pandas' \n",
    "import pandas as pd \n",
    "\n",
    "# import 'Numpy' \n",
    "import numpy as np\n",
    "\n",
    "# import subpackage of Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# import 'Seaborn' \n",
    "import seaborn as sns\n",
    "\n",
    "# to suppress warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be648e-eedd-4217-99ac-e9026601608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns of the dataframe\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# display all rows of the dataframe\n",
    "pd.options.display.max_rows = None\n",
    " \n",
    "# to display the float values upto 6 decimal places     \n",
    "pd.options.display.float_format = '{:.6f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5271a-c32f-43ad-ab4e-f9db38e56c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import StandardScaler to perform scaling\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# import various functions from sklearn \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4232f6-f8dc-4d4c-8e18-fa8cdd268f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the plot size using 'rcParams'\n",
    "# once the plot size is set using 'rcParams', it sets the size of all the forthcoming plots in the file\n",
    "# pass width and height in inches to 'figure.figsize' \n",
    "plt.rcParams['figure.figsize'] = [15,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da74a9-89bf-4104-96ee-a3187047420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file\n",
    "# store the data in 'df_admissions'\n",
    "df_admissions = pd.read_csv('E:/training/GL training/PES/machine learning2/session3/Admission_predict-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21660c-0eff-483f-8a44-bee44ff59d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five observations using head()\n",
    "df_admissions.head()\n",
    "\n",
    "\n",
    "# use 'shape' to check the dimension of data\n",
    "df_admissions.shape\n",
    "\n",
    "# use 'dtypes' to check the data type of a variable\n",
    "df_admissions.dtypes\n",
    "\n",
    "# convert numerical variables to categorical (object) \n",
    "# use astype() to change the data type\n",
    "\n",
    "# change the data type of 'Research'\n",
    "df_admissions['Research'] = df_admissions['Research'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392c609-6dff-437a-9939-c66fbff256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck the data types using 'dtypes'\n",
    "df_admissions.dtypes\n",
    "\n",
    "# drop the column 'Serial No.' using drop()\n",
    "# 'axis = 1' drops the specified column\n",
    "df_admissions = df_admissions.drop('Serial No.', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d36cd8-3d6c-4562-8257-0c194e6db1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the independent numeric variables, we plot the histogram to check the distribution of the variables\n",
    "# Note: the hist() function considers the numeric variables only, by default\n",
    "# we drop the target variable using drop()\n",
    "# 'axis=1' drops the specified column\n",
    "df_admissions.drop('Chance of Admit', axis = 1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210cca8-70af-444c-b016-ca15691b4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# display the plot\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34120072-0f4f-4768-83cc-bc1efdf59c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the skewness for each numeric independent variable\n",
    "print('Skewness:')\n",
    "# we drop the target variable using drop()\n",
    "# 'axis=1' drops the specified column\n",
    "# skew() returns the coefficient of skewness for each variable\n",
    "df_admissions.drop('Chance of Admit', axis = 1).skew()\n",
    "\n",
    "# for the independent categoric variable, we plot the count plot to check the distribution of the variable 'Research'\n",
    "# use countplot() to plot the count of each label in the categorical variable \n",
    "sns.countplot(df_admissions.Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693ca77-42dc-47ea-a1fb-5fa9fbcc352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plot and axes labels\n",
    "# set text size using 'fontsize'\n",
    "plt.title('Count Plot for Categorical Variable (Research)', fontsize = 15)\n",
    "plt.xlabel('Research', fontsize = 15)\n",
    "plt.ylabel('Count', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8402785-3df7-4212-beed-7e4b9e36b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c0345-29f3-42bd-b89c-1775327f3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only the target variable\n",
    "df_target = df_admissions['Chance of Admit'].copy()\n",
    "\n",
    "# get counts of 0's and 1's in the 'Chance of Admit' variable\n",
    "df_target.value_counts()\n",
    "\n",
    "# plot the countplot of the variable 'Chance of Admit'\n",
    "sns.countplot(x = df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4f52d-06ce-418b-8a12-5e99ad23ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use below code to print the values in the graph\n",
    "# 'x' and 'y' gives position of the text\n",
    "# 's' is the text \n",
    "plt.text(x = -0.05, y = df_target.value_counts()[0] + 1, s = str(round((df_target.value_counts()[0])*100/len(df_target),2)) + '%')\n",
    "plt.text(x = 0.95, y = df_target.value_counts()[1] +1, s = str(round((df_target.value_counts()[1])*100/len(df_target),2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af80b5a-477c-4329-a7cf-7fde8737bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plot and axes labels\n",
    "# set text size using 'fontsize'\n",
    "plt.title('Count Plot for Target Variable (Chance of Admit)', fontsize = 15)\n",
    "plt.xlabel('Target Variable', fontsize = 15)\n",
    "plt.ylabel('Count', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695cf95-3e60-47c8-a27e-bd7a2bc371dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816eeba8-b5f9-4b1a-8e1f-1be0b2bfeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the variables on the basis of total null values in the variable\n",
    "# 'isnull().sum()' returns the number of missing values in each variable\n",
    "# 'ascending = False' sorts values in the descending order\n",
    "# the variable with highest number of missing values will appear first\n",
    "Total = df_admissions.isnull().sum().sort_values(ascending=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cd27c-ece4-4e52-a168-f2dc1c6a20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of missing values\n",
    "# 'ascending = False' sorts values in the descending order\n",
    "# the variable with highest percentage of missing values will appear first\n",
    "Percent = (df_admissions.isnull().sum()*100/df_admissions.isnull().count()).sort_values(ascending=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0c7e2-7b50-4296-b9cf-6cd6f0537feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the 'Total' and 'Percent' columns using 'concat' function\n",
    "# pass a list of column names in parameter 'keys' \n",
    "# 'axis = 1' concats along the columns\n",
    "missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    \n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884e337-3aa3-48e6-99ed-c4282ce99c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target variable 'Chance of Admit' in a dataframe 'df_target'\n",
    "df_target = df_admissions['Chance of Admit']\n",
    "\n",
    "# store all the independent variables in a dataframe 'df_feature' \n",
    "# drop the column 'Chance of Admit' using drop()\n",
    "# 'axis = 1' drops the specified column\n",
    "df_feature = df_admissions.drop('Chance of Admit', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6c382-bb60-49d5-9260-d18553137f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the numerical features in the dataset\n",
    "# 'select_dtypes' is used to select the variables with given data type\n",
    "# 'include = [np.number]' will include all the numerical variables\n",
    "df_num = df_feature.select_dtypes(include = [np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe198167-41b3-4009-aab7-4af202429dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical features\n",
    "df_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5e438-92c0-43e5-8d5c-2d41b9459187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the categorical features in the dataset\n",
    "# 'select_dtypes' is used to select the variables with given data type\n",
    "# 'include = [np.object]' will include all the categorical variables\n",
    "df_cat = df_feature.select_dtypes(include = ['object', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468039c-27a1-41d1-ae0e-6786a9ad64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical features\n",
    "df_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e995e46-b8b2-4eb0-ae59-c58475e2c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'get_dummies' from pandas to create dummy variables\n",
    "# use 'drop_first' to create (n-1) dummy variables\n",
    "dummy_var = pd.get_dummies(data = df_cat, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a8491-3d02-496f-a8df-484fa00a9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the standard scalar\n",
    "X_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92eef08-e3b3-40d1-8257-e6a145176445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all the numerical columns\n",
    "# standardize all the columns of the dataframe 'df_num'\n",
    "num_scaled = X_scaler.fit_transform(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dde8a7-514b-47c0-bcff-69814b2ceff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of scaled numerical variables\n",
    "# pass the required column names to the parameter 'columns'\n",
    "df_num_scaled = pd.DataFrame(num_scaled, columns = df_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f59e5-3bf0-4bc7-85fc-d945c9ae56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the dummy variables with numeric features to create a dataframe of all independent variables\n",
    "# 'axis=1' concats the dataframes along columns \n",
    "X = pd.concat([df_num_scaled, dummy_var], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a0995-077c-407e-ab47-f3b6b98a6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five observations\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7c8af-c381-4303-9a74-1a7f8bf91537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train subset and test subset\n",
    "# set 'random_state' to generate the same dataset each time you run the code \n",
    "# 'test_size' returns the proportion of data to be included in the testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_target, random_state = 10, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2043a7-55dd-4e51-84f1-f1338eb0742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of the train & test subset using 'shape'\n",
    "# print dimension of train set\n",
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b162e8-d55a-4cc0-a10c-58b84af0edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dimension of test set\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137d101-43d4-4d5e-8f70-c0705f955ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generalized function to calculate the performance metrics values for test set\n",
    "def get_test_report(model):\n",
    "    \n",
    "    # for test set:\n",
    "    # test_pred: prediction made by the model on the test dataset 'X_test'\n",
    "    # y_test: actual values of the target variable for the test dataset\n",
    "\n",
    "    # predict the output of the target variable from the test data \n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # return the classification report for test data\n",
    "    return(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba3972-4dc0-492a-a035-0ecabb5df3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a to plot a confusion matrix for the model\n",
    "def plot_confusion_matrix(model):\n",
    "    \n",
    "    # predict the target values using X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # create a confusion matrix\n",
    "    # pass the actual and predicted target values to the confusion_matrix()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # label the confusion matrix  \n",
    "    # pass the matrix as 'data'\n",
    "    # pass the required column names to the parameter, 'columns'\n",
    "    # pass the required row names to the parameter, 'index'\n",
    "    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n",
    "\n",
    "    # plot a heatmap to visualize the confusion matrix\n",
    "    # 'annot' prints the value of each grid \n",
    "    # 'fmt = d' returns the integer value in each grid\n",
    "    # 'cmap' assigns color to each grid\n",
    "    # as we do not require different colors for each grid in the heatmap,\n",
    "    # use 'ListedColormap' to assign the specified color to the grid\n",
    "    # 'cbar = False' will not return the color bar to the right side of the heatmap\n",
    "    # 'linewidths' assigns the width to the line that divides each grid\n",
    "    # 'annot_kws = {'size':25})' assigns the font size of the annotated text \n",
    "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n",
    "                linewidths = 0.1, annot_kws = {'size':25})\n",
    "\n",
    "    # set the font size of x-axis ticks using 'fontsize'\n",
    "    plt.xticks(fontsize = 20)\n",
    "\n",
    "    # set the font size of y-axis ticks using 'fontsize'\n",
    "    plt.yticks(fontsize = 20)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504d312-2e50-4f81-a206-d6221840bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to plot the ROC curve and print the ROC-AUC score\n",
    "def plot_roc(model):\n",
    "    \n",
    "    # predict the probability of target variable using X_test\n",
    "    # consider the probability of positive class by subsetting with '[:,1]'\n",
    "    y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n",
    "    # pass the actual target values and predicted probabilities to the function\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # plot the ROC curve\n",
    "    plt.plot(fpr, tpr)\n",
    "\n",
    "    # set limits for x and y axes\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "\n",
    "    # plot the straight line showing worst prediction for the model\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "    # add plot and axes labels\n",
    "    # set text size using 'fontsize'\n",
    "    plt.title('ROC curve for Admission Prediction Classifier', fontsize = 15)\n",
    "    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n",
    "    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n",
    "\n",
    "    # add the AUC score to the plot\n",
    "    # 'x' and 'y' gives position of the text\n",
    "    # 's' is the text \n",
    "    # use round() to round-off the AUC score upto 4 digits\n",
    "    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n",
    "\n",
    "    # plot the grid\n",
    "    plt.grid(True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6155a2-9d3e-4cb1-b298-e3f97257257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # instantiate the 'KNeighborsClassifier'\n",
    "# n_neighnors: number of neighbors to consider\n",
    "# default metric is minkowski, and with p=2 it is equivalent to the euclidean metric\n",
    "knn_classification = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b1f8a-2b96-472c-ab9d-feaff8a28ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using fit() on train data\n",
    "knn_model = knn_classification.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42960f17-8b74-4ded-9111-86867511a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to plot the confusion matrix\n",
    "# pass the knn model to the function\n",
    "plot_confusion_matrix(knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5daf2-15df-4ef6-a70e-b5315d161507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the performance measures on test data\n",
    "# call the function 'get_test_report'\n",
    "# pass the knn model to the function\n",
    "test_report = get_test_report(knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf35f24-95dc-47a7-ab43-b106d4226dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the performace measures\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6377ad-00fc-4c2c-9419-d9a54a72b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to plot the ROC curve\n",
    "# pass the knn model to the function\n",
    "plot_roc(knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361acf32-915e-441f-8200-b436b35b7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with hyperparameters and its values\n",
    "# n_neighnors: number of neighbors to consider\n",
    "# usually, we consider the odd value of 'n_neighnors' to avoid the equal number of nearest points with more than one class\n",
    "# pass the different distance metrics to the parameter, 'metric'\n",
    "tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n",
    "                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98530358-ff90-4212-9d25-0a7b8752ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the 'KNeighborsClassifier' \n",
    "knn_classification = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e280e-21ac-4272-9603-e2b0ab02c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GridSearchCV() to find the optimal value of the hyperparameters\n",
    "# estimator: pass the knn model\n",
    "# param_grid: pass the list 'tuned_parameters'\n",
    "# cv: number of folds in k-fold i.e. here cv = 5\n",
    "# scoring: pass the scoring parameter 'accuracy'\n",
    "knn_grid = GridSearchCV(estimator = knn_classification, \n",
    "                        param_grid = tuned_paramaters, \n",
    "                        cv = 5, \n",
    "                        scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d0ee0-b3fa-4024-be94-0738b7cdbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on X_train and y_train using fit()\n",
    "knn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35782c49-62b6-41b7-90b8-643e5a442680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters\n",
    "print('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4321771-4109-444f-aebb-5a18998a8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider an empty list to store error rate\n",
    "error_rate = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4fc3b-80ec-4f15-8d40-9a971f53e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for loop to build a knn model for each K\n",
    "for i in np.arange(1,25,2):\n",
    "    \n",
    "    # setup a knn classifier with k neighbors\n",
    "    # use the 'euclidean' metric \n",
    "    knn = KNeighborsClassifier(i, metric = 'euclidean')\n",
    "   \n",
    "    # fit the model using 'cross_val_score'\n",
    "    # pass the knn model as 'estimator'\n",
    "    # use 5-fold cross validation\n",
    "    score = cross_val_score(knn, X_train, y_train, cv = 5)\n",
    "    \n",
    "    # calculate the mean score\n",
    "    score = score.mean()\n",
    "    \n",
    "    # compute error rate \n",
    "    error_rate.append(1 - score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c34d9c-9bca-44fb-aa15-8c5336ef3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error_rate for different values of K \n",
    "plt.plot(range(1,25,2), error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb8e43-be37-4824-8321-820e31d4e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plot and axes labels\n",
    "# set text size using 'fontsize'\n",
    "plt.title('Error Rate', fontsize = 15)\n",
    "plt.xlabel('K', fontsize = 15)\n",
    "plt.ylabel('Error Rate', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb7033-d824-4c7e-8df6-9c37b52bbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the x-axis labels\n",
    "plt.xticks(np.arange(1, 25, step = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81928616-ef16-455b-ab21-2153f82eb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a vertical line across the minimum error rate\n",
    "plt.axvline(x = 17, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8556b-0383-47ba-95cc-28e551cea20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fc631-b351-414d-94b3-531ae3301d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the performance measures for test set for the model with best parameters\n",
    "# call the function 'get_test_report'\n",
    "# pass the knn model using GridSearch to the function\n",
    "print('Classification Report for test set: \\n', get_test_report(knn_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013d4a0-062f-43e5-83df-3937cc66de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to plot the ROC curve\n",
    "# pass the knn model to the function\n",
    "plot_roc(knn_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0f105-6955-4fe5-ac13-7fc646d48ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the 'GaussianNB'\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9d117-7620-426e-a322-ac1180144c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using fit() on train data\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59af87-5c9c-4ea8-823e-62007a97a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to plot the confusion matrix\n",
    "# pass the gaussian naive bayes model to the function\n",
    "plot_confusion_matrix(gnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05110c0-9ed2-4c09-a955-72b9bf96b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the performance measures on test data\n",
    "# call the function 'get_test_report'\n",
    "# pass the gaussian naive bayes model to the function\n",
    "test_report = get_test_report(gnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ecb26-047e-4125-859b-2b22443f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the performace measures\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d237f-7bf5-4d64-a2c4-e37c1ea3fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to plot the ROC curve\n",
    "# pass the gaussian naive bayes model to the function\n",
    "plot_roc(gnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d76b5-fa36-4b76-a4ad-c8923a6e1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "y_pred_prob_knn = knn_grid.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f866d67-f26f-4be2-81e5-1b11a5c63cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the roc_curve() returns the values for false positive rate, true positive rate and threshold\n",
    "# pass the actual target values and predicted probabilities to the function\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddc25f-92ef-45ab-81fc-9db997a106e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the AUC score to the plot\n",
    "auc_score_knn = roc_auc_score(y_test, y_pred_prob_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69beed45-3c45-4420-a9ce-564481aecb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='KNN Model (AUC Score = %0.4f)' % auc_score_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e874f6-35f1-431b-9f59-15b76d6cc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "y_pred_prob_gnb = gnb_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211f3a4-8ff9-45ce-bd1b-f619715cbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the roc_curve() returns the values for false positive rate, true positive rate and threshold\n",
    "# pass the actual target values and predicted probabilities to the function\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb088d5-1f77-49dc-b1f5-2311dcd43dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the AUC score to the plot\n",
    "auc_score_gnb = roc_auc_score(y_test, y_pred_prob_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1da5e3-bfea-485a-bd20-97b070b0ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='GNB Model (AUC Score = %0.4f)' % auc_score_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6d80f-f7e7-4180-8e60-0e9c3c3de9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set limits for x and y axes\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3cb3f-1dfe-4b3e-9471-fca6d399c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the straight line showing worst prediction for the model\n",
    "plt.plot([0, 1], [0, 1],'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d0e30-8c9c-40b4-84d2-ba01c83c1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plot and axes labels\n",
    "# set text size using 'fontsize'\n",
    "plt.title('GNB Model Vs. KNN Model', fontsize = 15)\n",
    "plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n",
    "plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb366d3-78cb-4a6a-a63d-ec8873346f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the position of legend\n",
    "plt.legend(loc = 'lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76667ad2-4e44-40f1-9fad-3ba00375cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the grid\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
