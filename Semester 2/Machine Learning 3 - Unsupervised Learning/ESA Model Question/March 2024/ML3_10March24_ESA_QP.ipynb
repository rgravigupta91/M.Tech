{"cells":[{"cell_type":"markdown","metadata":{"id":"9sFPPSRZ2Uun"},"source":["### March 2024: END SEMESTER ASSESSMENT (ESA)\n","## M TECH DATA SCIENCE AND MACHINE LEARNING_ SEMESTER II\n","\n","### UE20CS932 - MACHINE LEARNING - III\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUD7Zgj6vpkH"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n","from sklearn.metrics import silhouette_score\n","from scipy.cluster.hierarchy import linkage , dendrogram, fcluster,cophenet\n","#from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from surprise import KNNWithMeans,SVDpp\n","from surprise import Dataset\n","from surprise import accuracy\n","from surprise import Reader\n","from surprise.model_selection import train_test_split,cross_validate\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules\n","from surprise import KNNBasic\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"1r9etSfQvpkP"},"source":["Note:\n","\n","Use Credit-Card-Dataset-for-Clustering.csv for all the clustering and dimensionality reduction questions.\n","\n","Use Book_ratings.csv for popularity-driven recommendation system questions and collaborative recommendation engine.\n","\n","Use Online Retail.csv for association rule mining using, apriori algorithm questions.  "]},{"cell_type":"raw","metadata":{"id":"sqUvigG72Wzk"},"source":["### problem statement:\n","\n","This case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 8950 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.\n","\n","### Data Description\n","-CUSTID : Identification of Credit Card holder (Categorical)\n","-BALANCE : Balance amount left in their account to make purchases\n","-BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n","-PURCHASES : Amount of purchases made from account\n","-ONEOFFPURCHASES : Maximum purchase amount done in one-go\n","-INSTALLMENTSPURCHASES : Amount of purchase done in installment\n","-CASHADVANCE : Cash in advance given by the user\n","-PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n","-ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n","-PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n","-CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n","-CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n","-PURCHASESTRX : Numbe of purchase transactions made\n","-CREDITLIMIT : Limit of Credit Card for user\n","-PAYMENTS : Amount of Payment done by user\n","-MINIMUM_PAYMENTS : Minimum amount of payments made by user\n","-PRCFULLPAYMENT : Percent of full payment paid by user\n","-TENURE : Tenure of credit card service for user\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pt7_ezKg2Wzk"},"source":["## Section B(40 marks)"]},{"cell_type":"markdown","metadata":{"id":"RDPXJL9h74UH"},"source":["#### 2. (a). Perform EDAand  pre-processing techniques (remove unnecessary variables, check the defects in the data) required for PCA and clustering. (10 marks) <br>   <br> Print the top 5 Eigenvalues and Eigenvectors. (4 marks)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaOiNp99j3B9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcCKZ5xi2Wzl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Iam2Z23e93EL"},"source":["##### 2. (b).Find the optimal number of clusters for the K-means clustering model [Note: Use the PCs, which are explaining the 90% variance].  (6 marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjGdVqY52Wzl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCH0uTGz2Wzl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"tqCGCDUC-mW-"},"source":["#### 2. (c). Plot the dendrograms using 4 linkage methods for the PCA transformed data and identify which one is the best. [Note: Use the PCs, which are explaining the 90% variance] (6 marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhazH91vkF_3"},"outputs":[],"source":["#Hint:Use the following functions and create the dendrograms for 100 nodes only\n","# linkage(pca_df, method='single',metric='euclidean')\n","# dendrogram(mergings,truncate_mode='lastp',p=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQimFVaz2Wzm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"buUKOxi9vpke"},"source":["#### 2 (d)Cluster the data into 4 groups and order the cluster quality in terms of the inertia (WCSS) of each cluster. [Use ward linkage metric] (6 marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Me2LP5_vvpke"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0r03Qk1_vpke"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"s7n3MC9C2Wzm"},"source":["#### 2 (e)Compare the quality of clusters for K-means clustering algorithm on original data and PCA transformed data. (8 marks)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZS4kISR2Wzm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27Gh988m2Wzn"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"3v4cgQO32Wzn"},"source":["## Section C (40 marks)"]},{"cell_type":"markdown","metadata":{"id":"60foq9YKvpkj"},"source":["\n","#### 3 (a) Develop a popularity-driven recommendation system, print Total no of ratings,Total No of Users,Total No of products  and recommend the top 5 items. (10 marks )\n","\n","Use the dataset: Book_ratings.csv\n","Variables:\n","book_id - ID of the book.\n","user_id - ID of the user rated\n","ratings - ratings of the book by the user."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXWiJC6XugCa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsjO9jQW5R5n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6tkwINg-vpkm"},"source":["\n","#### 3 (b) Build collaborative recommendation engine. Use KNNBasic library with cosine similarity and measure the model quality by performing cross validation in terms of RMSE.\n","Use the dataset: Book_ratings.csv (20 Marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXPTe7-c2Wzn"},"outputs":[],"source":["# Load the dataset\n","# Create a Reader object\n","# Load the dataset into Surprise's Dataset format (Hint: Dataset.load_from_df(df[['user_id', 'book_id', 'rating']], reader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5DAl9Fw2Wzn"},"outputs":[],"source":["# Split the data into training and testing sets\n","# Build the collaborative filtering model (user-based in this case)\n","#Hint: sim_options = {'name': 'cosine', 'user_based': True}\n","#model = KNNBasic(sim_options=sim_options)\n","# Train the model\n","# Make predictions on the test set\n","# Calculate RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2fas-jF2Wzn"},"outputs":[],"source":["# Perform cross-validation\n","\n","\n","# Print the average RMSE across folds\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1Y74oE22Wzo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_lXSrZL2Wzo"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jg6Ab6Rt2Wzo"},"source":["#### 3 (c) Create association rule mining using apriori algorithm.Perform basic pre-processing operations required by algorithm ( drop missing values, drop unnecessary columns). (7 marks) </br>  </br> Create the basket only for France. Run algorithm with minimum support 0.07, tune with lift. (3 marks)\n","\n","Use Dataset:Online Retail.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBcgb2SH2Wzo"},"outputs":[],"source":["#Read the data\n","#Check missing values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pO84QZkv2Wzo"},"outputs":[],"source":["#Hint : use the function to get frequesnt Itemsets\n","#apriori(basket for France, min_support, use_colnames=True)\n","#rules = association_rules(frequent_itemsets, metric=\"lift\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}