{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1720ba-fdce-4b3f-94df-c02a66c526c6",
   "metadata": {
    "id": "aa7a7a9f-98cf-4350-961a-08483c89bd32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhishek.satpathy\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhishek.satpathy\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhishek.satpathy\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "Device set to use 0\n",
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhishek.satpathy\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Device set to use 0\n",
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\abhishek.satpathy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhishek.satpathy\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "#  below code should not be executed by students ( only by IT team)\n",
    "pipe = pipeline(\"text-classification\")\n",
    "pipe.save_pretrained(\"classification_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('ner')\n",
    "pipe.save_pretrained(\"ner_pipeline_model\")\n",
    "\n",
    "\n",
    "pipe = pipeline('question-answering')\n",
    "pipe.save_pretrained(\"qa_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('fill-mask')\n",
    "pipe.save_pretrained(\"textgeneration_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('summarization')\n",
    "pipe.save_pretrained(\"summarizer_pipeline_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f4259",
   "metadata": {
    "id": "309f4259"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1650f757-1b35-4299-9af9-8183e8ab7dd6",
   "metadata": {
    "id": "1650f757-1b35-4299-9af9-8183e8ab7dd6"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d2837-3b7e-406f-aa22-5245082f87e0",
   "metadata": {
    "id": "8e6d2837-3b7e-406f-aa22-5245082f87e0"
   },
   "source": [
    "## 2. Use the data.csv dataset as provided as pandas DataFrame and  process the text feture as questioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84e476cf-61f1-431a-a027-f265045002ac",
   "metadata": {
    "id": "84e476cf-61f1-431a-a027-f265045002ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension is :  (134, 4)\n",
      "Dataset Smaple \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>user/id</th>\n",
       "      <th>user/name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-15T03:56:38.000Z</td>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>3f950a66a65bc31bcb9f76bfaeb25f5bbf60e7a79af124...</td>\n",
       "      <td>9aa68cfdb5e9231f14f7c5098fee94f99e59993791d218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-15T19:42:05.000Z</td>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>30ae350542e84e10d43998313e2833a6ba44c2bbc05dea...</td>\n",
       "      <td>6d39db2d3639841ed49bde0a3657e7dc9a6d2801ab5ba2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-02T16:16:25.000Z</td>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>ed3c46885cb5117d8cced8bc5a9b61e1c71347b0394177...</td>\n",
       "      <td>76e2cbaf54db202d7128581b24abde4228f7bf65a0e40b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-08T14:40:38.000Z</td>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>9ca9d18e6a177cb50144e424252339a2bc36893276e613...</td>\n",
       "      <td>a838ba1069854ce3ed3d6d7a18dafaa62b34dc46eb5ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-06T11:39:26.000Z</td>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>b4b2e6188f727101e7b2097100de4dfa345a525dd97d3c...</td>\n",
       "      <td>f26148e5f27721fbee260ce70d53c8a44801d67f01365d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0  2023-09-15T03:56:38.000Z   \n",
       "1  2023-09-15T19:42:05.000Z   \n",
       "2  2023-06-02T16:16:25.000Z   \n",
       "3  2023-05-08T14:40:38.000Z   \n",
       "4  2023-05-06T11:39:26.000Z   \n",
       "\n",
       "                                                text  \\\n",
       "0  The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1  We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2  I made an order today and I had to be waiting ...   \n",
       "3                      worst customer service ever ðŸ˜¡   \n",
       "4  McDonals Sri Lanka, in general does not delive...   \n",
       "\n",
       "                                             user/id  \\\n",
       "0  3f950a66a65bc31bcb9f76bfaeb25f5bbf60e7a79af124...   \n",
       "1  30ae350542e84e10d43998313e2833a6ba44c2bbc05dea...   \n",
       "2  ed3c46885cb5117d8cced8bc5a9b61e1c71347b0394177...   \n",
       "3  9ca9d18e6a177cb50144e424252339a2bc36893276e613...   \n",
       "4  b4b2e6188f727101e7b2097100de4dfa345a525dd97d3c...   \n",
       "\n",
       "                                           user/name  \n",
       "0  9aa68cfdb5e9231f14f7c5098fee94f99e59993791d218...  \n",
       "1  6d39db2d3639841ed49bde0a3657e7dc9a6d2801ab5ba2...  \n",
       "2  76e2cbaf54db202d7128581b24abde4228f7bf65a0e40b...  \n",
       "3  a838ba1069854ce3ed3d6d7a18dafaa62b34dc46eb5ebe...  \n",
       "4  f26148e5f27721fbee260ce70d53c8a44801d67f01365d...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "print(\"Dataset dimension is : \", df.shape)\n",
    "print(\"Dataset Smaple \", )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1987b6-153d-4272-943f-33dd71a7df01",
   "metadata": {
    "id": "eb1987b6-153d-4272-943f-33dd71a7df01"
   },
   "source": [
    "### 2.a.Pre-Process the text feature as questioned below. (in the same sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d6cbb-9a7d-492e-9941-a1d6baa16df1",
   "metadata": {
    "id": "a31d6cbb-9a7d-492e-9941-a1d6baa16df1"
   },
   "source": [
    "#### 2.a.1. Remove the accented characters from text feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79890bf4-e94f-43d5-b737-0f4f83e67d61",
   "metadata": {
    "id": "79890bf4-e94f-43d5-b737-0f4f83e67d61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>We didnt eat this we throw this to duspin.this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>worst customer service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Nice...</td>\n",
       "      <td>Nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>I &lt;3 it</td>\n",
       "      <td>I &lt;3 it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1    We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2    I made an order today and I had to be waiting ...   \n",
       "3                        worst customer service ever ðŸ˜¡   \n",
       "4    McDonals Sri Lanka, in general does not delive...   \n",
       "..                                                 ...   \n",
       "129            Need to introduce 'GRAND CHICKEN'......   \n",
       "130                                            Nice...   \n",
       "131                               Excellnet sevice! :)   \n",
       "132                                            I <3 it   \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...   \n",
       "\n",
       "                                   text_without_accent  \n",
       "0    The outlet in Nugegoda is terrible. Not sendin...  \n",
       "1    We didnt eat this we throw this to duspin.this...  \n",
       "2    I made an order today and I had to be waiting ...  \n",
       "3                         worst customer service ever   \n",
       "4    McDonals Sri Lanka, in general does not delive...  \n",
       "..                                                 ...  \n",
       "129            Need to introduce 'GRAND CHICKEN'......  \n",
       "130                                            Nice...  \n",
       "131                               Excellnet sevice! :)  \n",
       "132                                            I <3 it  \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...  \n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Eliminate Accented character\n",
    "## hint use below method\n",
    "def remove_accented_chars(text):\n",
    "     text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "     return text\n",
    "## usage:  remove_accented_chars('SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt')\n",
    "\n",
    "# Apply only the accented character removal function on the text column\n",
    "df['text_without_accent'] = df['text'].apply(remove_accented_chars)\n",
    "\n",
    "# Display the processed text data after removing accented characters\n",
    "df[['text', 'text_without_accent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf26e4b-3417-45f0-b8d2-cb1aa1c75f65",
   "metadata": {
    "id": "6cf26e4b-3417-45f0-b8d2-cb1aa1c75f65"
   },
   "source": [
    "#### 2.a.2. Remove stopwords from text feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5b633f1-ed0f-4781-9355-6fd5be9c0583",
   "metadata": {
    "id": "e5b633f1-ed0f-4781-9355-6fd5be9c0583"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_stopwords_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>outlet Nugegoda terrible. sending deliveries t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>didnt eat throw duspin.this crispy burger diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>made order today waiting 2 hours receive never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>worst customer service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>McDonals Sri Lanka, general deliver good food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "      <td>Need introduce 'GRAND CHICKEN'......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Nice...</td>\n",
       "      <td>Nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>I &lt;3 it</td>\n",
       "      <td>&lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "      <td>Kik June 12 @ july 13 WEEL COME FIFA world CUP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1    We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2    I made an order today and I had to be waiting ...   \n",
       "3                        worst customer service ever ðŸ˜¡   \n",
       "4    McDonals Sri Lanka, in general does not delive...   \n",
       "..                                                 ...   \n",
       "129            Need to introduce 'GRAND CHICKEN'......   \n",
       "130                                            Nice...   \n",
       "131                               Excellnet sevice! :)   \n",
       "132                                            I <3 it   \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...   \n",
       "\n",
       "                           text_without_stopwords_nltk  \n",
       "0    outlet Nugegoda terrible. sending deliveries t...  \n",
       "1    didnt eat throw duspin.this crispy burger diff...  \n",
       "2    made order today waiting 2 hours receive never...  \n",
       "3                          worst customer service ever  \n",
       "4    McDonals Sri Lanka, general deliver good food ...  \n",
       "..                                                 ...  \n",
       "129               Need introduce 'GRAND CHICKEN'......  \n",
       "130                                            Nice...  \n",
       "131                               Excellnet sevice! :)  \n",
       "132                                                 <3  \n",
       "133  Kik June 12 @ july 13 WEEL COME FIFA world CUP...  \n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## any of nltk or gensim can be used\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# Get stopwords dynamically\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords using NLTK\n",
    "def remove_stopwords_nltk(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word.lower() not in stop_words_nltk])\n",
    "\n",
    "# Apply the function\n",
    "df['text_without_stopwords_nltk'] = df['text_without_accent'].apply(remove_stopwords_nltk)\n",
    "\n",
    "df[[\"text\", \"text_without_stopwords_nltk\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a996492-56b4-4e17-bfb4-00a21553c4ea",
   "metadata": {
    "id": "6a996492-56b4-4e17-bfb4-00a21553c4ea"
   },
   "source": [
    "#### 2.a.3. Remove digits from text feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a3027fc-38e5-46bf-b273-3c3efcb5e609",
   "metadata": {
    "id": "1a3027fc-38e5-46bf-b273-3c3efcb5e609"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>outlet Nugegoda terrible. sending deliveries t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>didnt eat throw duspin.this crispy burger diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>made order today waiting  hours receive neverm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>worst customer service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>McDonals Sri Lanka, general deliver good food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "      <td>Need introduce 'GRAND CHICKEN'......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Nice...</td>\n",
       "      <td>Nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>I &lt;3 it</td>\n",
       "      <td>&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "      <td>Kik June  @ july  WEEL COME FIFA world CUP BRA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1    We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2    I made an order today and I had to be waiting ...   \n",
       "3                        worst customer service ever ðŸ˜¡   \n",
       "4    McDonals Sri Lanka, in general does not delive...   \n",
       "..                                                 ...   \n",
       "129            Need to introduce 'GRAND CHICKEN'......   \n",
       "130                                            Nice...   \n",
       "131                               Excellnet sevice! :)   \n",
       "132                                            I <3 it   \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...   \n",
       "\n",
       "                                   text_without_digits  \n",
       "0    outlet Nugegoda terrible. sending deliveries t...  \n",
       "1    didnt eat throw duspin.this crispy burger diff...  \n",
       "2    made order today waiting  hours receive neverm...  \n",
       "3                          worst customer service ever  \n",
       "4    McDonals Sri Lanka, general deliver good food ...  \n",
       "..                                                 ...  \n",
       "129               Need introduce 'GRAND CHICKEN'......  \n",
       "130                                            Nice...  \n",
       "131                               Excellnet sevice! :)  \n",
       "132                                                  <  \n",
       "133  Kik June  @ july  WEEL COME FIFA world CUP BRA...  \n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# remove digits\n",
    "#hint:  regular expression for digit matching is r'[0-9]'\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'[0-9]', '', text)\n",
    "\n",
    "# Apply the function to remove digits from the text column\n",
    "df['text_without_digits'] = df['text_without_stopwords_nltk'].apply(remove_digits)\n",
    "\n",
    "df[[\"text\", \"text_without_digits\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f2dcf-66c8-43b8-b8b0-ab33980c1ab0",
   "metadata": {
    "id": "d02f2dcf-66c8-43b8-b8b0-ab33980c1ab0"
   },
   "source": [
    "#### 2.a.4.\tRemove punctuations from text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32e0af56-d14f-49ca-b2d4-22463218b9ae",
   "metadata": {
    "id": "32e0af56-d14f-49ca-b2d4-22463218b9ae",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>outlet Nugegoda terrible sending deliveries ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>didnt eat throw duspinthis crispy burger diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>made order today waiting  hours receive neverm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>worst customer service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>McDonals Sri Lanka general deliver good food p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "      <td>Need introduce GRAND CHICKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Nice...</td>\n",
       "      <td>Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "      <td>Excellnet sevice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>I &lt;3 it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "      <td>Kik June   july  WEEL COME FIFA world CUP BRASIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1    We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2    I made an order today and I had to be waiting ...   \n",
       "3                        worst customer service ever ðŸ˜¡   \n",
       "4    McDonals Sri Lanka, in general does not delive...   \n",
       "..                                                 ...   \n",
       "129            Need to introduce 'GRAND CHICKEN'......   \n",
       "130                                            Nice...   \n",
       "131                               Excellnet sevice! :)   \n",
       "132                                            I <3 it   \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...   \n",
       "\n",
       "                              text_without_punctuation  \n",
       "0    outlet Nugegoda terrible sending deliveries ti...  \n",
       "1    didnt eat throw duspinthis crispy burger diffe...  \n",
       "2    made order today waiting  hours receive neverm...  \n",
       "3                          worst customer service ever  \n",
       "4    McDonals Sri Lanka general deliver good food p...  \n",
       "..                                                 ...  \n",
       "129                       Need introduce GRAND CHICKEN  \n",
       "130                                               Nice  \n",
       "131                                  Excellnet sevice   \n",
       "132                                                     \n",
       "133  Kik June   july  WEEL COME FIFA world CUP BRASIL   \n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  remove punchuations\n",
    "# hint: translate(str.maketrans('', '', string.punctuation))\n",
    "# Define function to remove punctuations\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df['text_without_punctuation'] = df['text_without_digits'].apply(remove_punctuation)\n",
    "\n",
    "df[[\"text\",\"text_without_punctuation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f5220-d349-4608-8157-561e6098f493",
   "metadata": {
    "id": "8f4f5220-d349-4608-8157-561e6098f493"
   },
   "source": [
    "#### 2.a.5.\tEliminate multiple spaces (by converting them as single space) from text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a33e02ed-66c2-4d06-95f5-0507e4a71bbc",
   "metadata": {
    "id": "a33e02ed-66c2-4d06-95f5-0507e4a71bbc"
   },
   "outputs": [],
   "source": [
    " # eliminate multiple spaces\n",
    "#hint: regular expression for spaces matching  is  r' +'\n",
    "# Define function to eliminate multiple spaces\n",
    "def eliminate_multiple_spaces(text):\n",
    "    return re.sub(r' +', ' ', text).strip()\n",
    "\n",
    "df['final_processed_text'] = df['text_without_punctuation'].apply(eliminate_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50f43c21-e3ed-4bfa-a46b-a0c8944bed5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>final_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>outlet Nugegoda terrible sending deliveries ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We didnâ€™t eat this we throw this to duspin.thi...</td>\n",
       "      <td>didnt eat throw duspinthis crispy burger diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>made order today waiting hours receive nevermi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worst customer service ever ðŸ˜¡</td>\n",
       "      <td>worst customer service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>McDonals Sri Lanka general deliver good food p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Need to introduce 'GRAND CHICKEN'......</td>\n",
       "      <td>Need introduce GRAND CHICKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Nice...</td>\n",
       "      <td>Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Excellnet sevice! :)</td>\n",
       "      <td>Excellnet sevice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>I &lt;3 it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Kik of June 12 @ july 13 WEEL COME TO FIFA wor...</td>\n",
       "      <td>Kik June july WEEL COME FIFA world CUP BRASIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1    We didnâ€™t eat this we throw this to duspin.thi...   \n",
       "2    I made an order today and I had to be waiting ...   \n",
       "3                        worst customer service ever ðŸ˜¡   \n",
       "4    McDonals Sri Lanka, in general does not delive...   \n",
       "..                                                 ...   \n",
       "129            Need to introduce 'GRAND CHICKEN'......   \n",
       "130                                            Nice...   \n",
       "131                               Excellnet sevice! :)   \n",
       "132                                            I <3 it   \n",
       "133  Kik of June 12 @ july 13 WEEL COME TO FIFA wor...   \n",
       "\n",
       "                                  final_processed_text  \n",
       "0    outlet Nugegoda terrible sending deliveries ti...  \n",
       "1    didnt eat throw duspinthis crispy burger diffe...  \n",
       "2    made order today waiting hours receive nevermi...  \n",
       "3                          worst customer service ever  \n",
       "4    McDonals Sri Lanka general deliver good food p...  \n",
       "..                                                 ...  \n",
       "129                       Need introduce GRAND CHICKEN  \n",
       "130                                               Nice  \n",
       "131                                   Excellnet sevice  \n",
       "132                                                     \n",
       "133      Kik June july WEEL COME FIFA world CUP BRASIL  \n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text\",\"final_processed_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c87496-c624-469a-9bbb-0e05552526ea",
   "metadata": {
    "id": "95c87496-c624-469a-9bbb-0e05552526ea"
   },
   "source": [
    "### 2.b.Find out the 5 most frequent words in the text corpus ((from the preprocessed output of previous question )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9794bf79-57ac-49cb-9c70-dc5648435f66",
   "metadata": {
    "id": "9794bf79-57ac-49cb-9c70-dc5648435f66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 50),\n",
       " ('order', 44),\n",
       " ('service', 41),\n",
       " ('customer', 36),\n",
       " ('McDonalds', 26)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all text into a single string\n",
    "corpus = ' '.join(df['final_processed_text'])\n",
    "\n",
    "# Tokenize words and count frequency\n",
    "word_counts = Counter(corpus.split())\n",
    "\n",
    "# Get the 5 most common words\n",
    "most_common_words = word_counts.most_common(5)\n",
    "\n",
    "# Display the most frequent words\n",
    "most_common_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e9885-9f5a-4903-951d-2fbd4b984495",
   "metadata": {
    "id": "958e9885-9f5a-4903-951d-2fbd4b984495"
   },
   "source": [
    "### 2.c. Vectorize the pre-processed text feature by building/training a Skip-Gram Word2Vec model. Use this Skip-Gram Word2Vec model to fetch the top 5 most similar word for the word 'food'.  (marks 3+ 5 =8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95d7c6cd-0ca4-440b-8d6c-c6e723cbf08d",
   "metadata": {
    "id": "95d7c6cd-0ca4-440b-8d6c-c6e723cbf08d"
   },
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text into a list of word lists\n",
    "tokens_list = [row.split() for row in df['final_processed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4589018c-1e55-46a0-910e-db5bf737099f",
   "metadata": {
    "id": "4589018c-1e55-46a0-910e-db5bf737099f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3793, 30190)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize cleaned text by building a Skip-Gram Word2Vec model.\n",
    "\n",
    "# # initialize skipgram model\n",
    "sg_model = Word2Vec(min_count=2,\n",
    "                    window=2, sg = 1,\n",
    "                    sample=5e-5, alpha=0.05,\n",
    "                    min_alpha=0.0005,negative=20 )\n",
    "\n",
    "# build model vocabulary\n",
    "sg_model.build_vocab(tokens_list)\n",
    "\n",
    "# train the model\n",
    "sg_model.train(tokens_list, total_examples=sg_model.corpus_count, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1c1ccc5-643a-4775-b5a7-d6250cbc1c56",
   "metadata": {
    "id": "c1c1ccc5-643a-4775-b5a7-d6250cbc1c56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('got', 0.996441125869751),\n",
       " ('time', 0.9955507516860962),\n",
       " ('order', 0.9952718615531921),\n",
       " ('manager', 0.9952440857887268),\n",
       " ('never', 0.9950111508369446)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use trained Skip-Gram Word2Vec model to fetch the top 5 most similar word for the word 'food'\n",
    "# Fetch the top 5 most similar words for 'food'\n",
    "similar_words = sg_model.wv.most_similar('food', topn=5)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea3ba8-3d3f-460e-b747-9a1be2895bc5",
   "metadata": {
    "id": "c5ea3ba8-3d3f-460e-b747-9a1be2895bc5"
   },
   "source": [
    "### 2.d. Vectorize the pre-processed text feature by building a CBOW Word2Vec model. Use the trained CBOW Word2Vec model to fetch the top 5 most similar word for the word 'food'. Is the output different than Skip-Gramâ€™s output? ( marks 3+5+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67012d7d-ef5e-42d6-84bd-62f6e837ca23",
   "metadata": {
    "id": "67012d7d-ef5e-42d6-84bd-62f6e837ca23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3793, 30190)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize cleaned text by building a CBOW Word2Vec model.\n",
    "\n",
    "\n",
    "# initialize\n",
    "cbow_model = Word2Vec(min_count=2,window=2, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005,\n",
    "                     negative=20 )\n",
    "\n",
    "# build model vocabulary\n",
    "cbow_model.build_vocab(tokens_list)\n",
    "\n",
    "# train the model\n",
    "cbow_model.train(tokens_list, total_examples=cbow_model.corpus_count, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f38aaa0-1e80-4df3-b7dc-d2946ca60d3a",
   "metadata": {
    "id": "2f38aaa0-1e80-4df3-b7dc-d2946ca60d3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('got', 0.869615912437439),\n",
       " ('time', 0.854473352432251),\n",
       " ('order', 0.8494972586631775),\n",
       " ('manager', 0.8299447894096375),\n",
       " ('mins', 0.8252116441726685)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the trained CBOW Word2Vec model to fetch the top 5 most similar word for the word 'food'\n",
    "# Fetch the top 5 most similar words for 'food' using CBOW model\n",
    "similar_words_cbow = cbow_model.wv.most_similar('food', topn=5)\n",
    "\n",
    "# Display the most similar words\n",
    "similar_words_cbow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d36cce-d513-401c-9f87-e920089ea2b8",
   "metadata": {
    "id": "89d36cce-d513-401c-9f87-e920089ea2b8"
   },
   "source": [
    "## 3. The task specific pretrained transformers pipeline models is saved and provided. Use them to perform below text processing tasks as questioned\n",
    "\n",
    "Below is the list of <b>model_name : task-name </b>\n",
    "\n",
    "- classification_pipeline_model : text-classification\n",
    "- ner_pipeline_model : ner\n",
    "- qa_pipeline_model : question-answering\n",
    "- textgeneration_pipeline_model: fill-mask\n",
    "- summarizer_pipeline_model : summarization\n",
    "\n",
    "hint :  command to build a user-specified model pipeline is ...\n",
    "`pipeline(\"<task-name>\", model=\"<model_name>\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557e60a-cca1-4cad-b6b5-0dee64079a40",
   "metadata": {
    "id": "8557e60a-cca1-4cad-b6b5-0dee64079a40"
   },
   "source": [
    "### 3.a. Using Sentence Classification - Sentiment Analysis model `classification_pipeline_model`, classify the sentence `Such a nice weather outside!` into positive/negative with score. (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d5da84-b4e2-4b2b-9dab-7c30b79f6bc1",
   "metadata": {
    "id": "29d5da84-b4e2-4b2b-9dab-7c30b79f6bc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at classification_pipeline_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997655749320984}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence classification\n",
    "\n",
    "# Load the sentiment analysis pipeline model\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"classification_pipeline_model\")\n",
    "\n",
    "# Classify the given sentence\n",
    "sentence = \"Such a nice weather outside!\"\n",
    "sentiment_result = sentiment_pipeline(sentence)\n",
    "\n",
    "# Display the sentiment classification result\n",
    "sentiment_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda017dc-df04-4ee6-b625-5b2364db3a52",
   "metadata": {
    "id": "eda017dc-df04-4ee6-b625-5b2364db3a52"
   },
   "source": [
    "### 3.b. Using Named Entity Recognition model `ner_pipeline_model`, perform name-entity -recognition of sentence  -`Hugging Face is a French company based in New-York.`  (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24fc9707-8850-4d60-9f5b-c3fb495cd1e7",
   "metadata": {
    "id": "24fc9707-8850-4d60-9f5b-c3fb495cd1e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at ner_pipeline_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-ORG', 'score': 0.9970939, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}, {'entity': 'I-ORG', 'score': 0.93457514, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}, {'entity': 'I-ORG', 'score': 0.97870606, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}, {'entity': 'I-MISC', 'score': 0.9981996, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}, {'entity': 'I-LOC', 'score': 0.9983047, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}, {'entity': 'I-LOC', 'score': 0.89134544, 'index': 11, 'word': '-', 'start': 45, 'end': 46}, {'entity': 'I-LOC', 'score': 0.99795234, 'index': 12, 'word': 'York', 'start': 46, 'end': 50}]\n"
     ]
    }
   ],
   "source": [
    "# ner\n",
    "# Load the NER pipeline model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"ner_pipeline_model\")\n",
    "\n",
    "# Sentence to analyze\n",
    "sentence = \"Hugging Face is a French company based in New-York.\"\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "ner_result = ner_pipeline(sentence)\n",
    "\n",
    "# Print the extracted named entities\n",
    "print(ner_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4b35a-6c60-41a0-a925-4f86642f7bef",
   "metadata": {
    "id": "67c4b35a-6c60-41a0-a925-4f86642f7bef"
   },
   "source": [
    "### 3.c. Using the Question Answering model `qa_pipeline_model`, provide the answer of  `question` asked from the given `paragraph` (for question and paragraph refer notebook). (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f87022-9cbb-459a-bbaf-2d6f33321088",
   "metadata": {
    "id": "73f87022-9cbb-459a-bbaf-2d6f33321088"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at qa_pipeline_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9711989164352417, 'start': 42, 'end': 50, 'answer': 'New-York'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Question Answering model pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"qa_pipeline_model\")\n",
    "\n",
    "paragraph = 'Hugging Face is a French company based in New-York.'\n",
    "question = 'Where is Hugging Face based?'\n",
    "\n",
    "# question answering\n",
    "# Perform Question Answering\n",
    "qa_result = qa_pipeline(question=question, context=paragraph)\n",
    "\n",
    "# Print the answer\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce6c5e-4158-403e-98fd-09fc64991224",
   "metadata": {
    "id": "8dce6c5e-4158-403e-98fd-09fc64991224"
   },
   "source": [
    "### 3.d. Using Text Generation - Mask Filling model `tg_pipeline_model`, suggest the appropriate words for specified `MISSING_WORD_Field`  in the  given sentence. (8 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162ae0e4-3a45-4d46-a35d-2764d4c08d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('fill-mask')\n",
    "pipe.save_pretrained(\"tg_pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1415dacc-51c2-4695-bc88-3c14fc78dd8b",
   "metadata": {
    "id": "1415dacc-51c2-4695-bc88-3c14fc78dd8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at tg_pipeline_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.26723772287368774, 'token': 8117, 'token_str': ' patterns', 'sequence': 'In Machine Learning, Machine learns patterns and biases.'}, {'score': 0.09592798352241516, 'token': 16964, 'token_str': ' algorithms', 'sequence': 'In Machine Learning, Machine learns algorithms and biases.'}, {'score': 0.05764855071902275, 'token': 17156, 'token_str': ' behaviors', 'sequence': 'In Machine Learning, Machine learns behaviors and biases.'}, {'score': 0.03544700890779495, 'token': 3650, 'token_str': ' behavior', 'sequence': 'In Machine Learning, Machine learns behavior and biases.'}, {'score': 0.022768452763557434, 'token': 31681, 'token_str': ' biases', 'sequence': 'In Machine Learning, Machine learns biases and biases.'}]\n"
     ]
    }
   ],
   "source": [
    "#sentence  =  'In Machine Learning, Machine learns ' \"MISSING_WORD_Field\"  'and biases.'\n",
    "\n",
    "# fill mask\n",
    "# Load the Fill-Mask pipeline model\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=\"tg_pipeline_model\")\n",
    "\n",
    "# Define the masked sentence\n",
    "sentence = \"In Machine Learning, Machine learns <mask> and biases.\"\n",
    "\n",
    "# Perform masked word prediction\n",
    "mask_result = fill_mask_pipeline(sentence)\n",
    "\n",
    "# Print the top suggested words\n",
    "print(mask_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449c08e-e506-4a74-b5fc-e100b5c9b697",
   "metadata": {
    "id": "7449c08e-e506-4a74-b5fc-e100b5c9b697"
   },
   "source": [
    "### 3.e. Using Summarization model `summarizer_pipeline_model`,provide summarization of  the given `Long_Tennis_Article` as provided. (8 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004afbd-f721-4fc6-9d98-30adab9764c0",
   "metadata": {
    "id": "5004afbd-f721-4fc6-9d98-30adab9764c0"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2afdefa7-00e2-423e-be80-32b56981b8aa",
   "metadata": {
    "id": "2afdefa7-00e2-423e-be80-32b56981b8aa"
   },
   "outputs": [],
   "source": [
    "Long_Tennis_Article = \"\"\"\n",
    " Andy Murray  came close to giving himself some extra preparation time for his w\n",
    "edding next week before ensuring that he still has unfinished tennis business to\n",
    " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
    "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to\n",
    "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
    "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
    "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
    "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
    "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
    " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
    "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong\n",
    "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
    "ow British tennis players who might be agitated about his imminent arrival into\n",
    "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
    " should use the assimilation of the world number 83, originally from Slovenia, a\n",
    "s motivation to better themselves. At present any grumbles are happening in priv\n",
    "ate, and Bedene's present ineligibility for the Davis Cup team has made it less\n",
    "of an issue, although that could change if his appeal to play is allowed by the\n",
    "International Tennis Federation. Murray thinks anyone questioning the move, now\n",
    "it has become official, would be better working on getting their ranking closer\n",
    "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
    "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
    "s obviously the British number two, comfortably. 'So they can complain but the b\n",
    "est thing to do is use it in the right way and accept it for what it is, and try\n",
    " to use it as motivation whether they agree with it or not. He's British now so\n",
    "they've just got to deal with it. Murray stretches for a return after starting h\n",
    "is quarter final match slowly on the show court . Thiem held nothing back as he\n",
    "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
    "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
    "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
    ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
    "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
    "en as a positive for those guys using it to try to get better. He's a good playe\n",
    "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
    "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
    "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
    ": 'He's a good player with a very good serve. He's a legitimate top 100 player,\n",
    "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
    "our he wins matches, it's not like he turns up and always loses in the first rou\n",
    "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
    "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
    "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
    "ell. I would imagine he would keep moving up the rankings although I don't know\n",
    "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
    " seen him play loads, but when you serve as well as he does it helps. I would im\n",
    "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
    ".' It is understood the Lawn Tennis Association will give background support to\n",
    "his case regarding the Davis Cup but have made it clear that the onus is on him\n",
    "to lead the way. An official statement said: 'To have another player in the men'\n",
    "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
    "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
    "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
    "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
    "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
    "o life as he raced to a commanding lead in the second . The No 3 seed sent over\n",
    "a few glaring looks towards his team before winning the second set .\n",
    "\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d13791-b30b-4aff-ac99-a51ad8548e96",
   "metadata": {
    "id": "26d13791-b30b-4aff-ac99-a51ad8548e96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at summarizer_pipeline_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "# Load the Summarization pipeline model\n",
    "summarization_pipeline = pipeline(\"summarization\", model=\"summarizer_pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b5031d-a10c-47f2-8cbf-c39f0f21876f",
   "metadata": {
    "id": "a6b5031d-a10c-47f2-8cbf-c39f0f21876f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6-3, 6-1 win over Dominic Thiem in the second set . the world no 4 is into the semi-finals of the Miami Open . he is awaiting the winner from the last eight of the tournament .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary\n",
    "summary_result = summarization_pipeline(Long_Tennis_Article)\n",
    "\n",
    "# Display the summary\n",
    "summary_result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2f050-c34d-4c44-b01b-6847d0185b59",
   "metadata": {
    "id": "a7b2f050-c34d-4c44-b01b-6847d0185b59"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
