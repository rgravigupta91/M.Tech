{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f8f80-a764-440b-a71d-a63169b36dad",
   "metadata": {
    "id": "aa7a7a9f-98cf-4350-961a-08483c89bd32"
   },
   "source": [
    "#  below code should not be executed by students ( only by IT team)\n",
    "pipe = pipeline(\"text-classification\")\n",
    "pipe.save_pretrained(\"classification_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('ner')\n",
    "pipe.save_pretrained(\"ner_pipeline_model\")\n",
    "\n",
    "\n",
    "pipe = pipeline('question-answering')\n",
    "pipe.save_pretrained(\"qa_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('fill-mask')\n",
    "pipe.save_pretrained(\"textgeneration_pipeline_model\")\n",
    "\n",
    "pipe = pipeline('summarization')\n",
    "pipe.save_pretrained(\"summarizer_pipeline_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f4259",
   "metadata": {
    "id": "309f4259"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1650f757-1b35-4299-9af9-8183e8ab7dd6",
   "metadata": {
    "id": "1650f757-1b35-4299-9af9-8183e8ab7dd6"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d2837-3b7e-406f-aa22-5245082f87e0",
   "metadata": {
    "id": "8e6d2837-3b7e-406f-aa22-5245082f87e0"
   },
   "source": [
    "## 2. Use the data.csv dataset as provided as pandas DataFrame and  process the text feture as questioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e476cf-61f1-431a-a027-f265045002ac",
   "metadata": {
    "id": "84e476cf-61f1-431a-a027-f265045002ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension is :  (134, 4)\n",
      "Dataset Smaple \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>user/id</th>\n",
       "      <th>user/name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-15T03:56:38.000Z</td>\n",
       "      <td>The outlet in Nugegoda is terrible. Not sendin...</td>\n",
       "      <td>3f950a66a65bc31bcb9f76bfaeb25f5bbf60e7a79af124...</td>\n",
       "      <td>9aa68cfdb5e9231f14f7c5098fee94f99e59993791d218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-15T19:42:05.000Z</td>\n",
       "      <td>We didn‚Äôt eat this we throw this to duspin.thi...</td>\n",
       "      <td>30ae350542e84e10d43998313e2833a6ba44c2bbc05dea...</td>\n",
       "      <td>6d39db2d3639841ed49bde0a3657e7dc9a6d2801ab5ba2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-02T16:16:25.000Z</td>\n",
       "      <td>I made an order today and I had to be waiting ...</td>\n",
       "      <td>ed3c46885cb5117d8cced8bc5a9b61e1c71347b0394177...</td>\n",
       "      <td>76e2cbaf54db202d7128581b24abde4228f7bf65a0e40b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-08T14:40:38.000Z</td>\n",
       "      <td>worst customer service ever üò°</td>\n",
       "      <td>9ca9d18e6a177cb50144e424252339a2bc36893276e613...</td>\n",
       "      <td>a838ba1069854ce3ed3d6d7a18dafaa62b34dc46eb5ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-06T11:39:26.000Z</td>\n",
       "      <td>McDonals Sri Lanka, in general does not delive...</td>\n",
       "      <td>b4b2e6188f727101e7b2097100de4dfa345a525dd97d3c...</td>\n",
       "      <td>f26148e5f27721fbee260ce70d53c8a44801d67f01365d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0  2023-09-15T03:56:38.000Z   \n",
       "1  2023-09-15T19:42:05.000Z   \n",
       "2  2023-06-02T16:16:25.000Z   \n",
       "3  2023-05-08T14:40:38.000Z   \n",
       "4  2023-05-06T11:39:26.000Z   \n",
       "\n",
       "                                                text  \\\n",
       "0  The outlet in Nugegoda is terrible. Not sendin...   \n",
       "1  We didn‚Äôt eat this we throw this to duspin.thi...   \n",
       "2  I made an order today and I had to be waiting ...   \n",
       "3                      worst customer service ever üò°   \n",
       "4  McDonals Sri Lanka, in general does not delive...   \n",
       "\n",
       "                                             user/id  \\\n",
       "0  3f950a66a65bc31bcb9f76bfaeb25f5bbf60e7a79af124...   \n",
       "1  30ae350542e84e10d43998313e2833a6ba44c2bbc05dea...   \n",
       "2  ed3c46885cb5117d8cced8bc5a9b61e1c71347b0394177...   \n",
       "3  9ca9d18e6a177cb50144e424252339a2bc36893276e613...   \n",
       "4  b4b2e6188f727101e7b2097100de4dfa345a525dd97d3c...   \n",
       "\n",
       "                                           user/name  \n",
       "0  9aa68cfdb5e9231f14f7c5098fee94f99e59993791d218...  \n",
       "1  6d39db2d3639841ed49bde0a3657e7dc9a6d2801ab5ba2...  \n",
       "2  76e2cbaf54db202d7128581b24abde4228f7bf65a0e40b...  \n",
       "3  a838ba1069854ce3ed3d6d7a18dafaa62b34dc46eb5ebe...  \n",
       "4  f26148e5f27721fbee260ce70d53c8a44801d67f01365d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "print(\"Dataset dimension is : \", df.shape)\n",
    "print(\"Dataset Smaple \", )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1987b6-153d-4272-943f-33dd71a7df01",
   "metadata": {
    "id": "eb1987b6-153d-4272-943f-33dd71a7df01"
   },
   "source": [
    "### 2.a.Pre-Process the text feature as questioned below. (in the same sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d6cbb-9a7d-492e-9941-a1d6baa16df1",
   "metadata": {
    "id": "a31d6cbb-9a7d-492e-9941-a1d6baa16df1"
   },
   "source": [
    "#### 2.a.1. Remove the accented characters from text feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79890bf4-e94f-43d5-b737-0f4f83e67d61",
   "metadata": {
    "id": "79890bf4-e94f-43d5-b737-0f4f83e67d61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Eliminate Accented character\n",
    "## hint use below method\n",
    "def remove_accented_chars(text):\n",
    "     text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "     return text\n",
    "## usage:  remove_accented_chars('S√≥mƒõ √Åccƒõntƒõd tƒõxt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf26e4b-3417-45f0-b8d2-cb1aa1c75f65",
   "metadata": {
    "id": "6cf26e4b-3417-45f0-b8d2-cb1aa1c75f65"
   },
   "source": [
    "#### 2.a.2. Remove stopwords from text feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b633f1-ed0f-4781-9355-6fd5be9c0583",
   "metadata": {
    "id": "e5b633f1-ed0f-4781-9355-6fd5be9c0583"
   },
   "outputs": [],
   "source": [
    "## any of nltk or gensim can be used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a996492-56b4-4e17-bfb4-00a21553c4ea",
   "metadata": {
    "id": "6a996492-56b4-4e17-bfb4-00a21553c4ea"
   },
   "source": [
    "#### 2.a.3. Remove digits from text feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3027fc-38e5-46bf-b273-3c3efcb5e609",
   "metadata": {
    "id": "1a3027fc-38e5-46bf-b273-3c3efcb5e609"
   },
   "outputs": [],
   "source": [
    "# remove digits\n",
    "#hint:  regular expression for digit matching is r'[0-9]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f2dcf-66c8-43b8-b8b0-ab33980c1ab0",
   "metadata": {
    "id": "d02f2dcf-66c8-43b8-b8b0-ab33980c1ab0"
   },
   "source": [
    "#### 2.a.4.\tRemove punctuations from text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0af56-d14f-49ca-b2d4-22463218b9ae",
   "metadata": {
    "id": "32e0af56-d14f-49ca-b2d4-22463218b9ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  remove punchuations\n",
    "# hint: translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f5220-d349-4608-8157-561e6098f493",
   "metadata": {
    "id": "8f4f5220-d349-4608-8157-561e6098f493"
   },
   "source": [
    "#### 2.a.5.\tEliminate multiple spaces (by converting them as single space) from text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e02ed-66c2-4d06-95f5-0507e4a71bbc",
   "metadata": {
    "id": "a33e02ed-66c2-4d06-95f5-0507e4a71bbc"
   },
   "outputs": [],
   "source": [
    " # eliminate multiple spaces\n",
    "#hint: regular expression for spaces matching  is  r' +'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c87496-c624-469a-9bbb-0e05552526ea",
   "metadata": {
    "id": "95c87496-c624-469a-9bbb-0e05552526ea"
   },
   "source": [
    "### 2.b.Find out the 5 most frequent words in the text corpus ((from the preprocessed output of previous question )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794bf79-57ac-49cb-9c70-dc5648435f66",
   "metadata": {
    "id": "9794bf79-57ac-49cb-9c70-dc5648435f66",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958e9885-9f5a-4903-951d-2fbd4b984495",
   "metadata": {
    "id": "958e9885-9f5a-4903-951d-2fbd4b984495"
   },
   "source": [
    "### 2.c. Vectorize the pre-processed text feature by building/training a Skip-Gram Word2Vec model. Use this Skip-Gram Word2Vec model to fetch the top 5 most similar word for the word 'food'.  (marks 3+ 5 =8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7c6cd-0ca4-440b-8d6c-c6e723cbf08d",
   "metadata": {
    "id": "95d7c6cd-0ca4-440b-8d6c-c6e723cbf08d"
   },
   "outputs": [],
   "source": [
    "# hint : refer  below code to fetch the token list\n",
    "tokens_list = [row.split() for row in df['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589018c-1e55-46a0-910e-db5bf737099f",
   "metadata": {
    "id": "4589018c-1e55-46a0-910e-db5bf737099f"
   },
   "outputs": [],
   "source": [
    "# Vectorize cleaned text by building a Skip-Gram Word2Vec model.\n",
    "\n",
    "# # initialize skipgram model\n",
    "sg_model = Word2Vec(min_count=2,\n",
    "                    window=2, sg = 1,\n",
    "                    sample=5e-5, alpha=0.05,\n",
    "                    min_alpha=0.0005,negative=20 )\n",
    "\n",
    "# build model vocabulary\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1ccc5-643a-4775-b5a7-d6250cbc1c56",
   "metadata": {
    "id": "c1c1ccc5-643a-4775-b5a7-d6250cbc1c56"
   },
   "outputs": [],
   "source": [
    "#Use trained Skip-Gram Word2Vec model to fetch the top 5 most similar word for the word 'food'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea3ba8-3d3f-460e-b747-9a1be2895bc5",
   "metadata": {
    "id": "c5ea3ba8-3d3f-460e-b747-9a1be2895bc5"
   },
   "source": [
    "### 2.d. Vectorize the pre-processed text feature by building a CBOW Word2Vec model. Use the trained CBOW Word2Vec model to fetch the top 5 most similar word for the word 'food'. Is the output different than Skip-Gram‚Äôs output? ( marks 3+5+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67012d7d-ef5e-42d6-84bd-62f6e837ca23",
   "metadata": {
    "id": "67012d7d-ef5e-42d6-84bd-62f6e837ca23"
   },
   "outputs": [],
   "source": [
    "# Vectorize cleaned text by building a CBOW Word2Vec model.\n",
    "\n",
    "\n",
    "# initialize\n",
    "cbow_model = Word2Vec(min_count=2,window=2, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005,\n",
    "                     negative=20 )\n",
    "# build model vocabulary\n",
    "\n",
    "\n",
    "# train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38aaa0-1e80-4df3-b7dc-d2946ca60d3a",
   "metadata": {
    "id": "2f38aaa0-1e80-4df3-b7dc-d2946ca60d3a"
   },
   "outputs": [],
   "source": [
    "# Use the trained CBOW Word2Vec model to fetch the top 5 most similar word for the word 'food'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d36cce-d513-401c-9f87-e920089ea2b8",
   "metadata": {
    "id": "89d36cce-d513-401c-9f87-e920089ea2b8"
   },
   "source": [
    "## 3. The task specific pretrained transformers pipeline models is saved and provided. Use them to perform below text processing tasks as questioned\n",
    "\n",
    "Below is the list of <b>model_name : task-name </b>\n",
    "\n",
    "- classification_pipeline_model : text-classification\n",
    "- ner_pipeline_model : ner\n",
    "- qa_pipeline_model : question-answering\n",
    "- textgeneration_pipeline_model: fill-mask\n",
    "- summarizer_pipeline_model : summarization\n",
    "\n",
    "hint :  command to build a user-specified model pipeline is ...\n",
    "`pipeline(\"<task-name>\", model=\"<model_name>\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557e60a-cca1-4cad-b6b5-0dee64079a40",
   "metadata": {
    "id": "8557e60a-cca1-4cad-b6b5-0dee64079a40"
   },
   "source": [
    "### 3.a. Using Sentence Classification - Sentiment Analysis model `classification_pipeline_model`, classify the sentence `Such a nice weather outside!` into positive/negative with score. (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5da84-b4e2-4b2b-9dab-7c30b79f6bc1",
   "metadata": {
    "id": "29d5da84-b4e2-4b2b-9dab-7c30b79f6bc1"
   },
   "outputs": [],
   "source": [
    "# sentence classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda017dc-df04-4ee6-b625-5b2364db3a52",
   "metadata": {
    "id": "eda017dc-df04-4ee6-b625-5b2364db3a52"
   },
   "source": [
    "### 3.b. Using Named Entity Recognition model `ner_pipeline_model`, perform name-entity -recognition of sentence  -`Hugging Face is a French company based in New-York.`  (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc9707-8850-4d60-9f5b-c3fb495cd1e7",
   "metadata": {
    "id": "24fc9707-8850-4d60-9f5b-c3fb495cd1e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4b35a-6c60-41a0-a925-4f86642f7bef",
   "metadata": {
    "id": "67c4b35a-6c60-41a0-a925-4f86642f7bef"
   },
   "source": [
    "### 3.c. Using the Question Answering model `qa_pipeline_model`, provide the answer of  `question` asked from the given `paragraph` (for question and paragraph refer notebook). (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f87022-9cbb-459a-bbaf-2d6f33321088",
   "metadata": {
    "id": "73f87022-9cbb-459a-bbaf-2d6f33321088"
   },
   "outputs": [],
   "source": [
    "paragraph = 'Hugging Face is a French company based in New-York.'\n",
    "question = 'Where is Hugging Face based?'\n",
    "\n",
    "# question answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce6c5e-4158-403e-98fd-09fc64991224",
   "metadata": {
    "id": "8dce6c5e-4158-403e-98fd-09fc64991224"
   },
   "source": [
    "### 3.d. Using Text Generation - Mask Filling model `tg_pipeline_model`, suggest the appropriate words for specified `MISSING_WORD_Field`  in the  given sentence. (8 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415dacc-51c2-4695-bc88-3c14fc78dd8b",
   "metadata": {
    "id": "1415dacc-51c2-4695-bc88-3c14fc78dd8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sentence  =  'In Machine Learning, Machine learns ' \"MISSING_WORD_Field\"  'and biases.'\n",
    "\n",
    "# fill mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449c08e-e506-4a74-b5fc-e100b5c9b697",
   "metadata": {
    "id": "7449c08e-e506-4a74-b5fc-e100b5c9b697"
   },
   "source": [
    "### 3.e. Using Summarization model `summarizer_pipeline_model`,provide summarization of  the given `Long_Tennis_Article` as provided. (8 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004afbd-f721-4fc6-9d98-30adab9764c0",
   "metadata": {
    "id": "5004afbd-f721-4fc6-9d98-30adab9764c0"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdefa7-00e2-423e-be80-32b56981b8aa",
   "metadata": {
    "id": "2afdefa7-00e2-423e-be80-32b56981b8aa"
   },
   "outputs": [],
   "source": [
    "Long_Tennis_Article = \"\"\"\n",
    " Andy Murray  came close to giving himself some extra preparation time for his w\n",
    "edding next week before ensuring that he still has unfinished tennis business to\n",
    " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
    "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to\n",
    "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
    "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
    "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
    "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
    "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
    " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
    "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong\n",
    "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
    "ow British tennis players who might be agitated about his imminent arrival into\n",
    "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
    " should use the assimilation of the world number 83, originally from Slovenia, a\n",
    "s motivation to better themselves. At present any grumbles are happening in priv\n",
    "ate, and Bedene's present ineligibility for the Davis Cup team has made it less\n",
    "of an issue, although that could change if his appeal to play is allowed by the\n",
    "International Tennis Federation. Murray thinks anyone questioning the move, now\n",
    "it has become official, would be better working on getting their ranking closer\n",
    "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
    "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
    "s obviously the British number two, comfortably. 'So they can complain but the b\n",
    "est thing to do is use it in the right way and accept it for what it is, and try\n",
    " to use it as motivation whether they agree with it or not. He's British now so\n",
    "they've just got to deal with it. Murray stretches for a return after starting h\n",
    "is quarter final match slowly on the show court . Thiem held nothing back as he\n",
    "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
    "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
    "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
    ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
    "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
    "en as a positive for those guys using it to try to get better. He's a good playe\n",
    "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
    "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
    "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
    ": 'He's a good player with a very good serve. He's a legitimate top 100 player,\n",
    "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
    "our he wins matches, it's not like he turns up and always loses in the first rou\n",
    "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
    "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
    "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
    "ell. I would imagine he would keep moving up the rankings although I don't know\n",
    "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
    " seen him play loads, but when you serve as well as he does it helps. I would im\n",
    "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
    ".' It is understood the Lawn Tennis Association will give background support to\n",
    "his case regarding the Davis Cup but have made it clear that the onus is on him\n",
    "to lead the way. An official statement said: 'To have another player in the men'\n",
    "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
    "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
    "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
    "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
    "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
    "o life as he raced to a commanding lead in the second . The No 3 seed sent over\n",
    "a few glaring looks towards his team before winning the second set .\n",
    "\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d13791-b30b-4aff-ac99-a51ad8548e96",
   "metadata": {
    "id": "26d13791-b30b-4aff-ac99-a51ad8548e96"
   },
   "outputs": [],
   "source": [
    "# summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5031d-a10c-47f2-8cbf-c39f0f21876f",
   "metadata": {
    "id": "a6b5031d-a10c-47f2-8cbf-c39f0f21876f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2f050-c34d-4c44-b01b-6847d0185b59",
   "metadata": {
    "id": "a7b2f050-c34d-4c44-b01b-6847d0185b59"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
