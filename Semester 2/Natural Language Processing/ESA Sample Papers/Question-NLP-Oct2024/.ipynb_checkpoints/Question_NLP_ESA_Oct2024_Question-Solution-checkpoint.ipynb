{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "nvjoO5CWI03T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "import numpy                  as np\n",
    "import pandas                 as pd\n",
    "import matplotlib.pyplot      as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "\n",
    "from time                                        import time\n",
    "from transformers                                import pipeline\n",
    "\n",
    "import gensim\n",
    "from gensim.models                               import Word2Vec\n",
    "from gensim.parsing.preprocessing                import remove_stopwords\n",
    "\n",
    "from nltk.corpus                                 import stopwords\n",
    "from nltk.tokenize                               import word_tokenize\n",
    "\n",
    "from collections                                 import Counter\n",
    "\n",
    "from tensorflow.keras.models                     import Sequential\n",
    "from tensorflow.keras.layers                     import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence     import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text         import Tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text  import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics                  import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from sklearn.naive_bayes              import MultinomialNB\n",
    "from sklearn                          import preprocessing\n",
    "from sklearn.preprocessing            import LabelEncoder\n",
    "\n",
    "from keras.utils                      import to_categorical\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize a simple pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test the pipeline\n",
    "result = nlp(\"I love using Hugging Face Transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "UqnmzG-uKhgD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review Sentiment\n",
      "0  This product exceeded my expectations! It's hi...  Positive\n",
      "1  The product was decent. It worked fine, but it...   Neutral\n",
      "2  I had a terrible experience with this company....  Negative\n",
      "3  It's an okay product. Nothing to write home ab...   Neutral\n",
      "4  Disappointed with the product. It didn't meet ...  Negative\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCEaJDV9JV59"
   },
   "source": [
    "Section B: Text Preprocessing and Word Embedding (40 Marks)\n",
    "\n",
    "Question 2: Text Preprocessing (20 Marks)\n",
    "\n",
    "Given the dataset reviews.csv, perform the following preprocessing steps:\n",
    "\n",
    "(a) Load the dataset and display the first 5 rows. (5 Marks)\n",
    "\n",
    "(b) Remove any duplicate reviews. (5 Marks)\n",
    "\n",
    "(c) Clean the text by removing punctuation, converting to lowercase, and removing stopwords. (10 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 2)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji\n",
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['Review'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product exceeded my expectations! It's hi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product was decent. It worked fine, but it...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had a terrible experience with this company....</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's an okay product. Nothing to write home ab...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disappointed with the product. It didn't meet ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment\n",
       "0  This product exceeded my expectations! It's hi...  Positive\n",
       "1  The product was decent. It worked fine, but it...   Neutral\n",
       "2  I had a terrible experience with this company....  Negative\n",
       "3  It's an okay product. Nothing to write home ab...   Neutral\n",
       "4  Disappointed with the product. It didn't meet ...  Negative"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "Nzf8DU5RJYxW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review Sentiment  \\\n",
      "0  This product exceeded my expectations! It's hi...  Positive   \n",
      "1  The product was decent. It worked fine, but it...   Neutral   \n",
      "2  I had a terrible experience with this company....  Negative   \n",
      "3  It's an okay product. Nothing to write home ab...   Neutral   \n",
      "4  Disappointed with the product. It didn't meet ...  Negative   \n",
      "\n",
      "                                      Cleaned_Review  \n",
      "0  product exceeded expectations highquality perf...  \n",
      "1   product decent worked fine want anything special  \n",
      "2  terrible experience company customer service r...  \n",
      "3                    okay product nothing write home  \n",
      "4             disappointed product meet expectations  \n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=['Review'],keep='first',inplace=True)\n",
    "\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # Remove Multiple Spaces\n",
    "    text = re.sub(r' +',' ', string=text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Remove HTML Tags\n",
    "    pattern = re.compile('<.*?>')\n",
    "    text = pattern.sub(r'',text)\n",
    "\n",
    "    #Remove URLs\n",
    "    pattern = re.compile(r'https?://S+|www.\\.\\S+')\n",
    "    text = pattern.sub(r'',text)\n",
    "\n",
    "    #Remove Emoji/Replace\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Chat Word conversion\n",
    "\n",
    "    # Remoce Accented Characters\n",
    "    unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "\n",
    "    # Spelling Correction\n",
    "    textBlob = TextBlob(text)\n",
    "    text = textBlob.correct().string\n",
    "\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence.append(word)\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    # \n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function\n",
    "\n",
    "df['Cleaned_Review'] = df['Review'].apply(clean_text) #clean_text(df['Review'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"   # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"   # symbols and pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"   # transport and map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"   # flags (ios)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming / Lemmatization\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "all_words = []\n",
    "\n",
    "def steme(text):\n",
    "    return \" \".join([porter.stem(word) for word in text.split()])\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize(text):\n",
    "    all_words = []\n",
    "    sentence_words = nltk.word_tokenize(text)\n",
    "    for word in sentence_words:\n",
    "        all_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return \" \".join(all_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Review'] = df['Cleaned_Review'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product exceeded my expectations! It's hi...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>product exceeded expectation highquality perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product was decent. It worked fine, but it...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>product decent worked fine want anything special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had a terrible experience with this company....</td>\n",
       "      <td>Negative</td>\n",
       "      <td>terrible experience company customer service r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's an okay product. Nothing to write home ab...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>okay product nothing write home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disappointed with the product. It didn't meet ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>disappointed product meet expectation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>The delivery was late and the packaging was da...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>delivery late packing damaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Avoid this company. Terrible customer service.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>avoid company terrible customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Neutral product. Didn't meet expectations.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>neutral product meet expectation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Avoid this product. It's a complete waste of m...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>avoid product complete waste money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>My experience was neither good nor bad. The pr...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>experience neither good bad product average se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review Sentiment  \\\n",
       "0    This product exceeded my expectations! It's hi...  Positive   \n",
       "1    The product was decent. It worked fine, but it...   Neutral   \n",
       "2    I had a terrible experience with this company....  Negative   \n",
       "3    It's an okay product. Nothing to write home ab...   Neutral   \n",
       "4    Disappointed with the product. It didn't meet ...  Negative   \n",
       "..                                                 ...       ...   \n",
       "356  The delivery was late and the packaging was da...  Negative   \n",
       "365     Avoid this company. Terrible customer service.  Negative   \n",
       "376         Neutral product. Didn't meet expectations.   Neutral   \n",
       "380  Avoid this product. It's a complete waste of m...  Negative   \n",
       "385  My experience was neither good nor bad. The pr...   Neutral   \n",
       "\n",
       "                                        Cleaned_Review  \n",
       "0    product exceeded expectation highquality perfo...  \n",
       "1     product decent worked fine want anything special  \n",
       "2    terrible experience company customer service r...  \n",
       "3                      okay product nothing write home  \n",
       "4                disappointed product meet expectation  \n",
       "..                                                 ...  \n",
       "356                      delivery late packing damaged  \n",
       "365            avoid company terrible customer service  \n",
       "376                   neutral product meet expectation  \n",
       "380                 avoid product complete waste money  \n",
       "385  experience neither good bad product average se...  \n",
       "\n",
       "[131 rows x 3 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idyNoW1hKKWA"
   },
   "source": [
    "Question 3: Word Embedding (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into word embeddings using TF-IDF(10 Marks)\n",
    "\n",
    "(b) Display the shape of the resulting TF-IDF matrix. (4 Marks)\n",
    "\n",
    "(c) Get the vocabulary of TF-IDF  (6 marks)\n",
    "\n",
    "\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "R3JAs3DdKTbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 153)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vect = vectorizer.fit_transform(df['Cleaned_Review'])\n",
    "\n",
    "# Display the shape of the TF-IDF matrix\n",
    "vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 107,\n",
       " 'exceeded': 46,\n",
       " 'expectation': 50,\n",
       " 'highquality': 67,\n",
       " 'perform': 101,\n",
       " 'exceptionally': 49,\n",
       " 'well': 144,\n",
       " 'decent': 31,\n",
       " 'worked': 148,\n",
       " 'fine': 57,\n",
       " 'want': 142,\n",
       " 'anything': 5,\n",
       " 'special': 130,\n",
       " 'terrible': 137,\n",
       " 'experience': 52,\n",
       " 'company': 24,\n",
       " 'customer': 29,\n",
       " 'service': 128,\n",
       " 'rude': 125,\n",
       " 'helpful': 65,\n",
       " 'okay': 94,\n",
       " 'nothing': 93,\n",
       " 'write': 152,\n",
       " 'home': 68,\n",
       " 'disappointed': 37,\n",
       " 'meet': 85,\n",
       " 'avoid': 10,\n",
       " 'cost': 26,\n",
       " 'supper': 135,\n",
       " 'outstanding': 95,\n",
       " 'exactly': 45,\n",
       " 'looking': 80,\n",
       " 'work': 147,\n",
       " 'perfectly': 100,\n",
       " 'absolutely': 0,\n",
       " 'horrendous': 69,\n",
       " 'never': 91,\n",
       " 'topquality': 140,\n",
       " 'satisfied': 127,\n",
       " 'purchase': 112,\n",
       " 'extremely': 54,\n",
       " 'dissatisfied': 40,\n",
       " 'cheaply': 22,\n",
       " 'made': 83,\n",
       " 'function': 59,\n",
       " 'properly': 111,\n",
       " 'satisfactory': 126,\n",
       " 'either': 42,\n",
       " 'would': 151,\n",
       " 'recommend': 117,\n",
       " 'overpriced': 97,\n",
       " 'advertised': 1,\n",
       " 'exceptional': 48,\n",
       " 'representative': 121,\n",
       " 'friendly': 58,\n",
       " 'great': 62,\n",
       " 'quality': 114,\n",
       " 'average': 9,\n",
       " 'particularly': 99,\n",
       " 'good': 61,\n",
       " 'bad': 14,\n",
       " 'poor': 104,\n",
       " 'worth': 150,\n",
       " 'price': 106,\n",
       " 'highly': 66,\n",
       " 'blow': 17,\n",
       " 'away': 11,\n",
       " 'recommended': 118,\n",
       " 'fantastic': 55,\n",
       " 'knowledgeable': 77,\n",
       " 'excellent': 47,\n",
       " 'anyone': 4,\n",
       " 'mediocre': 84,\n",
       " 'job': 76,\n",
       " 'awful': 12,\n",
       " 'expected': 51,\n",
       " 'item': 75,\n",
       " 'money': 87,\n",
       " 'neither': 89,\n",
       " 'worst': 149,\n",
       " 'give': 60,\n",
       " 'ever': 44,\n",
       " 'received': 116,\n",
       " 'faulty': 56,\n",
       " 'responsive': 123,\n",
       " 'neutral': 90,\n",
       " 'stand': 132,\n",
       " 'better': 15,\n",
       " 'impressive': 72,\n",
       " 'horrible': 70,\n",
       " 'atrocious': 7,\n",
       " 'regret': 119,\n",
       " 'buying': 21,\n",
       " 'poorly': 105,\n",
       " 'impressed': 71,\n",
       " 'definitely': 32,\n",
       " 'return': 124,\n",
       " 'incredible': 73,\n",
       " 'prompt': 109,\n",
       " 'needed': 88,\n",
       " 'could': 27,\n",
       " 'happier': 63,\n",
       " 'purchasing': 113,\n",
       " 'love': 81,\n",
       " 'met': 86,\n",
       " 'remarkable': 120,\n",
       " 'amazing': 3,\n",
       " 'buy': 20,\n",
       " 'dreadful': 41,\n",
       " 'staff': 131,\n",
       " 'shop': 129,\n",
       " 'stay': 133,\n",
       " 'brilliant': 19,\n",
       " 'went': 146,\n",
       " 'beyond': 16,\n",
       " 'arrived': 6,\n",
       " 'promptly': 110,\n",
       " 'wellpackaged': 145,\n",
       " 'pleasantly': 103,\n",
       " 'surprised': 136,\n",
       " 'attention': 8,\n",
       " 'detail': 36,\n",
       " 'delivery': 34,\n",
       " 'late': 78,\n",
       " 'damaged': 30,\n",
       " 'loved': 82,\n",
       " 'superb': 134,\n",
       " 'like': 79,\n",
       " 'plague': 102,\n",
       " 'disappointing': 38,\n",
       " 'professional': 108,\n",
       " 'craftsmanship': 28,\n",
       " 'overall': 96,\n",
       " 'time': 138,\n",
       " 'described': 35,\n",
       " 'noteworthy': 92,\n",
       " 'topnotch': 139,\n",
       " 'encountered': 43,\n",
       " 'extraordinary': 53,\n",
       " 'delighted': 33,\n",
       " 'help': 64,\n",
       " 'issue': 74,\n",
       " 'amazed': 2,\n",
       " 'unable': 141,\n",
       " 'complete': 25,\n",
       " 'waste': 143,\n",
       " 'come': 23,\n",
       " 'back': 13,\n",
       " 'bought': 18,\n",
       " 'disappointment': 39,\n",
       " 'resolved': 122,\n",
       " 'quickly': 115,\n",
       " 'packing': 98}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolutely', 'advertised', 'amazed', 'amazing', 'anyone',\n",
       "       'anything', 'arrived', 'atrocious', 'attention', 'average',\n",
       "       'avoid', 'away', 'awful', 'back', 'bad', 'better', 'beyond',\n",
       "       'blow', 'bought', 'brilliant', 'buy', 'buying', 'cheaply', 'come',\n",
       "       'company', 'complete', 'cost', 'could', 'craftsmanship',\n",
       "       'customer', 'damaged', 'decent', 'definitely', 'delighted',\n",
       "       'delivery', 'described', 'detail', 'disappointed', 'disappointing',\n",
       "       'disappointment', 'dissatisfied', 'dreadful', 'either',\n",
       "       'encountered', 'ever', 'exactly', 'exceeded', 'excellent',\n",
       "       'exceptional', 'exceptionally', 'expectation', 'expected',\n",
       "       'experience', 'extraordinary', 'extremely', 'fantastic', 'faulty',\n",
       "       'fine', 'friendly', 'function', 'give', 'good', 'great', 'happier',\n",
       "       'help', 'helpful', 'highly', 'highquality', 'home', 'horrendous',\n",
       "       'horrible', 'impressed', 'impressive', 'incredible', 'issue',\n",
       "       'item', 'job', 'knowledgeable', 'late', 'like', 'looking', 'love',\n",
       "       'loved', 'made', 'mediocre', 'meet', 'met', 'money', 'needed',\n",
       "       'neither', 'neutral', 'never', 'noteworthy', 'nothing', 'okay',\n",
       "       'outstanding', 'overall', 'overpriced', 'packing', 'particularly',\n",
       "       'perfectly', 'perform', 'plague', 'pleasantly', 'poor', 'poorly',\n",
       "       'price', 'product', 'professional', 'prompt', 'promptly',\n",
       "       'properly', 'purchase', 'purchasing', 'quality', 'quickly',\n",
       "       'received', 'recommend', 'recommended', 'regret', 'remarkable',\n",
       "       'representative', 'resolved', 'responsive', 'return', 'rude',\n",
       "       'satisfactory', 'satisfied', 'service', 'shop', 'special', 'staff',\n",
       "       'stand', 'stay', 'superb', 'supper', 'surprised', 'terrible',\n",
       "       'time', 'topnotch', 'topquality', 'unable', 'want', 'waste',\n",
       "       'well', 'wellpackaged', 'went', 'work', 'worked', 'worst', 'worth',\n",
       "       'would', 'write'], dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Review', 'Sentiment', 'Cleaned_Review'], dtype='object')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "LFEaz_iDKE83"
   },
   "outputs": [],
   "source": [
    "# tfidf vocabulaory\n",
    "#Hint: get feature names\n",
    "encoder = LabelEncoder()\n",
    "X = pd.DataFrame(data=vect.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "y = encoder.fit_transform(df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQiRLo4wKXm7"
   },
   "source": [
    "Section B: Model Building for Sentiment Analysis (40 Marks)\n",
    "\n",
    "Question 4: Naive Bayes Classifier (20 Marks)\n",
    "\n",
    "(a) Split the dataset into training and testing sets. (5 Marks)\n",
    "\n",
    "(b) Train a Naive Bayes classifier on the training set. (10 Marks)\n",
    "\n",
    "(c) Evaluate the model on the testing set and display the accuracy. (5 Marks)\n",
    "\n",
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "Qf3KRQFaKk6j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629629629629629"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sPRuBr_KqBB"
   },
   "source": [
    "Question 5: LSTM Model (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into sequences using Tokenizer. (5 Marks)\n",
    "\n",
    "(b) Build and compile an LSTM model for sentiment analysis. (10 Marks)\n",
    "\n",
    "(c) Train the model and evaluate its performance on the testing set. (5 Marks)\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "CEhUSS_uKwG5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Python\\Anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.3300 - loss: 0.5636 - val_accuracy: 0.2593 - val_loss: 0.2756\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3240 - loss: 0.0293 - val_accuracy: 0.2593 - val_loss: 0.3833\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.3630 - loss: -0.3112 - val_accuracy: 0.2593 - val_loss: 0.5110\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3345 - loss: 0.0554 - val_accuracy: 0.2593 - val_loss: 0.5494\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.3275 - loss: -0.0168 - val_accuracy: 0.2593 - val_loss: 0.5522\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3470 - loss: -0.1707 - val_accuracy: 0.2593 - val_loss: 0.5630\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3832 - loss: -0.1162 - val_accuracy: 0.2593 - val_loss: 0.5872\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3042 - loss: -0.4498 - val_accuracy: 0.2593 - val_loss: 0.6457\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.3098 - loss: -0.2683 - val_accuracy: 0.2593 - val_loss: 0.6811\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2973 - loss: -0.3972 - val_accuracy: 0.2593 - val_loss: 0.7181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23d30bbac90>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['Cleaned_Review'])\n",
    "\n",
    "#Hint : Use texts_to_sequences function\n",
    "sequences = tokenizer.texts_to_sequences(df['Cleaned_Review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length = 100),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.3487 - loss: -0.7895 - val_accuracy: 0.2593 - val_loss: 0.7365\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.3788 - loss: 0.4141 - val_accuracy: 0.2593 - val_loss: 0.7358\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3296 - loss: -0.4021 - val_accuracy: 0.2593 - val_loss: 0.7683\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3442 - loss: -1.4903 - val_accuracy: 0.2593 - val_loss: 0.7925\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3353 - loss: -0.3473 - val_accuracy: 0.2593 - val_loss: 0.7854\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3479 - loss: 0.1227 - val_accuracy: 0.2593 - val_loss: 0.7898\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2703 - loss: -1.0053 - val_accuracy: 0.2593 - val_loss: 0.8157\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.3410 - loss: -0.3389 - val_accuracy: 0.2593 - val_loss: 0.8352\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2998 - loss: -1.2918 - val_accuracy: 0.2593 - val_loss: 0.8618\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2910 - loss: -0.8333 - val_accuracy: 0.2593 - val_loss: 0.8837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23d34162e50>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.3242 - loss: -0.1547 - val_accuracy: 0.2593 - val_loss: 0.8166\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3357 - loss: -0.1316 - val_accuracy: 0.2593 - val_loss: 0.8301\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3346 - loss: 0.6614 - val_accuracy: 0.2593 - val_loss: 0.8481\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3200 - loss: -0.6686 - val_accuracy: 0.2593 - val_loss: 0.8763\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.3492 - loss: -0.3207 - val_accuracy: 0.2593 - val_loss: 0.8932\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3075 - loss: -0.7605 - val_accuracy: 0.2593 - val_loss: 0.9081\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.3284 - loss: -0.7387 - val_accuracy: 0.2593 - val_loss: 0.9312\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3357 - loss: -1.2897 - val_accuracy: 0.2593 - val_loss: 0.9527\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.3450 - loss: -0.2913 - val_accuracy: 0.2593 - val_loss: 0.9633\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3294 - loss: -0.3779 - val_accuracy: 0.2593 - val_loss: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23d2b04dd50>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3124 - loss: -2.0608 - val_accuracy: 0.2593 - val_loss: 1.3124\n",
      "Epoch 2/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3244 - loss: 0.1059 - val_accuracy: 0.2593 - val_loss: 1.3098\n",
      "Epoch 3/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2983 - loss: -0.3396 - val_accuracy: 0.2593 - val_loss: 1.3168\n",
      "Epoch 4/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3160 - loss: -1.0567 - val_accuracy: 0.2593 - val_loss: 1.3210\n",
      "Epoch 5/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4115 - loss: -1.5662 - val_accuracy: 0.2593 - val_loss: 1.3254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23d33b6e210>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25925925374031067"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    45\n",
       "Negative    44\n",
       "Neutral     42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
