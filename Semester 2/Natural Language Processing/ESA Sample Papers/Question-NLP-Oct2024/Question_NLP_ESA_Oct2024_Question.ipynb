{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvjoO5CWI03T"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqnmzG-uKhgD"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCEaJDV9JV59"
   },
   "source": [
    "Section B: Text Preprocessing and Word Embedding (40 Marks)\n",
    "\n",
    "Question 2: Text Preprocessing (20 Marks)\n",
    "\n",
    "Given the dataset reviews.csv, perform the following preprocessing steps:\n",
    "\n",
    "(a) Load the dataset and display the first 5 rows. (5 Marks)\n",
    "\n",
    "(b) Remove any duplicate reviews. (5 Marks)\n",
    "\n",
    "(c) Clean the text by removing punctuation, converting to lowercase, and removing stopwords. (10 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Nzf8DU5RJYxW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Remove duplicates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    #use translate function\n",
    "    # hint: refer string.punctuation to fetch all punctuations\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to lowercase\n",
    "\n",
    "\n",
    "\n",
    "    # Remove stopwords\n",
    "    # you need to import word_tokenize\n",
    "    from nltk.tokenize import word_tokenize # import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return text\n",
    "\n",
    "# Apply the cleaning function\n",
    "\n",
    "df['Cleaned_Review'] = df['Review'].apply(clean_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idyNoW1hKKWA"
   },
   "source": [
    "Question 3: Word Embedding (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into word embeddings using TF-IDF(10 Marks)\n",
    "\n",
    "(b) Display the shape of the resulting TF-IDF matrix. (4 Marks)\n",
    "\n",
    "(c) Get the vocabulary of TF-IDF  (6 marks)\n",
    "\n",
    "\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3JAs3DdKTbc"
   },
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the shape of the TF-IDF matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFEaz_iDKE83"
   },
   "outputs": [],
   "source": [
    "# tfidf vocabulaory\n",
    "#Hint: get feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQiRLo4wKXm7"
   },
   "source": [
    "Section B: Model Building for Sentiment Analysis (40 Marks)\n",
    "\n",
    "Question 4: Naive Bayes Classifier (20 Marks)\n",
    "\n",
    "(a) Split the dataset into training and testing sets. (5 Marks)\n",
    "\n",
    "(b) Train a Naive Bayes classifier on the training set. (10 Marks)\n",
    "\n",
    "(c) Evaluate the model on the testing set and display the accuracy. (5 Marks)\n",
    "\n",
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf3KRQFaKk6j"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "# Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sPRuBr_KqBB"
   },
   "source": [
    "Question 5: LSTM Model (20 Marks)\n",
    "\n",
    "(a) Convert the cleaned text into sequences using Tokenizer. (5 Marks)\n",
    "\n",
    "(b) Build and compile an LSTM model for sentiment analysis. (10 Marks)\n",
    "\n",
    "(c) Train the model and evaluate its performance on the testing set. (5 Marks)\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEhUSS_uKwG5"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['Cleaned_Review'])\n",
    "\n",
    "#Hint : Use texts_to_sequences function\n",
    "\n",
    "\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
