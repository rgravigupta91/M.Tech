{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99087378-5eb1-49ed-a3dd-4f32c3a72e9f",
   "metadata": {},
   "source": [
    "<table align=\"left\" width=100%>\n",
    "    <tr>\n",
    "        <td width=\"15%\">\n",
    "            <!-- <img src=\"faculty.png\"> -->\n",
    "        </td>\n",
    "        <td>\n",
    "            <div align=\"center\">\n",
    "                <font color=\"#21618C\" size=8px>\n",
    "                  <b> Faculty Notebook <br> (NLP Session 4 ) </b>\n",
    "                </font>\n",
    "            </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49288503-46aa-4d9f-9629-6c9de41f2009",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. **[Hugging Face Pipeline](#Hugface)**  \n",
    "    - 1.1 [Sentence Classification- Sentiment Analysis](#sc)\n",
    "    - 1.2 [Named Entity Recognition](#ner)\n",
    "    - 1.3 [Question Answering](#qa)\n",
    "    - 1.4 [Text Generation - Mask Filling](#tg)\n",
    "    - 1.5 [Summarization](#summ)\n",
    "    - 1.6 [Text Generation](#tg2)\n",
    "    - 1.7 [Projection - Features Extraction](#pfe) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55e511-44d6-4910-a52e-f51c5f51c649",
   "metadata": {},
   "source": [
    "<a id=\"Hugface\"> </a>\n",
    "## 1. Hugging Face Pipeline\n",
    "\n",
    "#### Introduced in transformers v2.3.0, **pipelines** provides a high-level, easy to use,API for doing inference over a variety of downstream-tasks, including: \n",
    "\n",
    "- ***Sentence Classification _(Sentiment Analysis)_***: Indicate if the overall sentence is either positive or negative, i.e. *binary classification task* or *logitic regression task*.\n",
    "- ***Token Classification (Named Entity Recognition, Part-of-Speech tagging)***: For each sub-entities _(*tokens*)_ in the input, assign them a label, i.e. classification task.\n",
    "- ***Question-Answering***: Provided a tuple (`question`, `context`) the model should find the span of text in `content` answering the `question`.\n",
    "- ***Mask-Filling***: Suggests possible word(s) to fill the masked input with respect to the provided `context`.\n",
    "- ***Summarization***: Summarizes the ``input`` article to a shorter article.\n",
    "- ***Feature Extraction***: Maps the input to a higher, multi-dimensional space learned from the data.\n",
    "\n",
    "Pipelines encapsulate the overall process of every NLP process:\n",
    " \n",
    " - 1. *Tokenization*: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).\n",
    " - 2. *Inference*: Maps every tokens into a more meaningful representation. \n",
    " - 3. *Decoding*: Use the above representation to generate and/or extract the final output for the underlying task.\n",
    "\n",
    "The overall API is exposed to the end-user through the `pipeline()` method with the following \n",
    "structure:\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Using default model and tokenizer for the task\n",
    "pipeline(\"<task-name>\")\n",
    "\n",
    "# Using a user-specified model\n",
    "pipeline(\"<task-name>\", model=\"<model_name>\")\n",
    "\n",
    "# Using custom model/tokenizer as str\n",
    "pipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n",
    "```\n",
    "\n",
    "Pre-trained models are available at https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5c36a3-5f8c-4730-8924-19ec4abbe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 22:16:37.970747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow -q\n",
    "\n",
    "#!pip install -q transformers\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22590ae2-51f3-4616-b399-ac730ed331ad",
   "metadata": {},
   "source": [
    "<a id=\"sc\"> </a>\n",
    "\n",
    "### 1.1. Sentence Classification - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab301ca4-1e2a-4c4c-8e74-253d2f18c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6a182151ba46b4acd3e77132baadcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5954666ea7c4939b36bba5bf8724f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8547802e5adc486385be587542978f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e20bd96fa34d798b36934ff4741ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656e302f672643abb1a7f6a6e43eff88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Initialize Sentiment analysis pipeline\n",
    "model_id = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "nlp_sentence_classif = pipeline('sentiment-analysis',model=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075e96ad-202d-404f-8c38-cc77861a8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997655749320984}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentence_classif('Such a nice weather outside !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9832a103-527d-47a8-8cc0-5550da04d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9841156601905823}]\n",
      "Time taken 0.08904409408569336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9091585874557495}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the text for classification\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(nlp_sentence_classif('Such a nice weather outside !'))\n",
    "print('Time taken', time.time() - start_time)\n",
    "\n",
    "#Feed the text for classification\n",
    "nlp_sentence_classif('The audio was good. But camera was not so good')\n",
    "\n",
    "#Feed the text for classification\n",
    "nlp_sentence_classif('That was not a nice movie')\n",
    "\n",
    "#Initialize Sentiment analysis pipeline\n",
    "#nlp_sentence_classif = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc220c05-873b-48dd-974f-0f77254d9a4a",
   "metadata": {},
   "source": [
    "<a id=\"ner\"> </a>\n",
    "### 1.2. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22084e8-c023-4a15-9d7d-22d4c11d2854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Initialize the pipeline for NER\n",
    "nlp_token_class = pipeline('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e829de-85cf-4230-878d-d2f708a7f93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_token_class.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d68055-c1d1-46e0-be62-f84745757280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-ORG',\n",
       "  'score': 0.9970939,\n",
       "  'index': 1,\n",
       "  'word': 'Hu',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.93457514,\n",
       "  'index': 2,\n",
       "  'word': '##gging',\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97870606,\n",
       "  'index': 3,\n",
       "  'word': 'Face',\n",
       "  'start': 8,\n",
       "  'end': 12},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9981996,\n",
       "  'index': 6,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9983047,\n",
       "  'index': 10,\n",
       "  'word': 'New',\n",
       "  'start': 42,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.89134544,\n",
       "  'index': 11,\n",
       "  'word': '-',\n",
       "  'start': 45,\n",
       "  'end': 46},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99795234,\n",
       "  'index': 12,\n",
       "  'word': 'York',\n",
       "  'start': 46,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the text for NER\n",
    "nlp_token_class('Hugging Face is a French company based in New-York.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5fe9e-f81e-46de-a01b-065e6c146c7c",
   "metadata": {},
   "source": [
    "<a id=\"qa\"> </a>\n",
    "### 1.3.  Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0860be7-b204-4c07-a7ae-b84a9d03df68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9711984992027283, 'start': 42, 'end': 50, 'answer': 'New-York'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize pipeline for Question Answering\n",
    "nlp_qa = pipeline('question-answering')\n",
    "#Feed a Paragraph and ask questions from the same\n",
    "paragraph = 'Hugging Face is a French company based in New-York.'\n",
    "question = 'Where is Hugging Face based?'\n",
    "nlp_qa(context=paragraph, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e602eaf0-4c34-4586-92f0-52123d978d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Google LLC is an American multinational technology company that specializes in Internet-related \n",
    "            services and products, which include online advertising technologies, search engine, \n",
    "            cloud computing, software, and hardware. It is considered one of the Big Four technology companies, \n",
    "            alongside Amazon, Apple, and Facebook. Google was founded in September 1998 by Larry Page and\n",
    "            Sergey Brin while they were Ph.D. students at Stanford University in California. Together they \n",
    "            own about 14 percent of its shares and control 56 percent of the stockholder voting power \n",
    "            through supervoting stock. They incorporated Google as a California privately held company \n",
    "            on September 4, 1998, in California. Google was then reincorporated in Delaware on October \n",
    "            22, 2002. An initial public offering (IPO) took place on August 19, 2004, and Google moved to \n",
    "            its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, \n",
    "            Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. \n",
    "            Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's \n",
    "            Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page who became \n",
    "            the CEO of Alphabet.\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9784f549-bb68-4ac8-8857-4d98cd560d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9617117047309875,\n",
       " 'start': 1286,\n",
       " 'end': 1299,\n",
       " 'answer': 'Sundar Pichai'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qa(context=article, question='Who is the CEO of Google?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d3a3c9-0c78-43e7-a144-b0f822b1995c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9250507354736328,\n",
       " 'start': 378,\n",
       " 'end': 392,\n",
       " 'answer': 'September 1998'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qa(context=article, question='When did Google start its operations?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f63283d-6401-47a8-868f-8a287d19e9ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8655408620834351,\n",
       " 'start': 396,\n",
       " 'end': 433,\n",
       " 'answer': 'Larry Page and            Sergey Brin'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qa(context=article, question='Who is the founder of Google?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e16ef7e0-bc9d-4bf2-aebc-16638f4406bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7243670225143433,\n",
       " 'start': 963,\n",
       " 'end': 988,\n",
       " 'answer': 'Mountain View, California'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qa(context=article, question='Where is Google office located?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bda5a3-fd03-4309-a970-9dc29f2319ba",
   "metadata": {},
   "source": [
    "<a id=\"tg\"> </a>\n",
    "### 1.4. Text Generation - Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e740d10-22b7-4a7e-900c-9fbe00751ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1999e2d9cfe84380a81fb91e5b341b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd327da7b054ba89330a5509893e241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46ee838927244289bde050b056fb805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d12b447d5f4dc6bafacc1bfe9696a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053f374fd33843dd86fddcfd0c9e532d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.27759018540382385,\n",
       "  'token': 2201,\n",
       "  'token_str': ' Paris',\n",
       "  'sequence': 'Hugging Face is a French company based in Paris'},\n",
       " {'score': 0.14941217005252838,\n",
       "  'token': 12790,\n",
       "  'token_str': ' Lyon',\n",
       "  'sequence': 'Hugging Face is a French company based in Lyon'},\n",
       " {'score': 0.04576420038938522,\n",
       "  'token': 11559,\n",
       "  'token_str': ' Geneva',\n",
       "  'sequence': 'Hugging Face is a French company based in Geneva'},\n",
       " {'score': 0.04576258733868599,\n",
       "  'token': 1470,\n",
       "  'token_str': ' France',\n",
       "  'sequence': 'Hugging Face is a French company based in France'},\n",
       " {'score': 0.04067569598555565,\n",
       "  'token': 6497,\n",
       "  'token_str': ' Brussels',\n",
       "  'sequence': 'Hugging Face is a French company based in Brussels'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize the pipeline\n",
    "nlp_fill = pipeline('fill-mask')\n",
    "#Provide a text with MASKed words that need to be filled up\n",
    "nlp_fill('Hugging Face is a French company based in ' + nlp_fill.tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "242da318-c356-43c7-8156-9daf84bf9509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2672387361526489,\n",
       "  'token': 8117,\n",
       "  'token_str': ' patterns',\n",
       "  'sequence': 'In Machine Learning, Machine learns patterns and biases.'},\n",
       " {'score': 0.09592907875776291,\n",
       "  'token': 16964,\n",
       "  'token_str': ' algorithms',\n",
       "  'sequence': 'In Machine Learning, Machine learns algorithms and biases.'},\n",
       " {'score': 0.057648662477731705,\n",
       "  'token': 17156,\n",
       "  'token_str': ' behaviors',\n",
       "  'sequence': 'In Machine Learning, Machine learns behaviors and biases.'},\n",
       " {'score': 0.035447217524051666,\n",
       "  'token': 3650,\n",
       "  'token_str': ' behavior',\n",
       "  'sequence': 'In Machine Learning, Machine learns behavior and biases.'},\n",
       " {'score': 0.022768709808588028,\n",
       "  'token': 31681,\n",
       "  'token_str': ' biases',\n",
       "  'sequence': 'In Machine Learning, Machine learns biases and biases.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill('In Machine Learning, Machine learns ' + nlp_fill.tokenizer.mask_token + ' and biases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3d47f-6fa6-49bf-bc24-86eb292412bc",
   "metadata": {},
   "source": [
    "<a id=\"summ\"> </a>\n",
    "### 1.5. Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "032603b5-c6ad-48e9-bbfe-12366adb5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#initialize pipeline\n",
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca1ff66d-b6f1-4cd9-9293-f12e27df4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_BORING_TENNIS_ARTICLE = \"\"\"\n",
    " Andy Murray  came close to giving himself some extra preparation time for his w\n",
    "edding next week before ensuring that he still has unfinished tennis business to\n",
    " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
    "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to \n",
    "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
    "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
    "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
    "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
    "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
    " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
    "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong \n",
    "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
    "ow British tennis players who might be agitated about his imminent arrival into \n",
    "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
    " should use the assimilation of the world number 83, originally from Slovenia, a\n",
    "s motivation to better themselves. At present any grumbles are happening in priv\n",
    "ate, and Bedene's present ineligibility for the Davis Cup team has made it less \n",
    "of an issue, although that could change if his appeal to play is allowed by the \n",
    "International Tennis Federation. Murray thinks anyone questioning the move, now \n",
    "it has become official, would be better working on getting their ranking closer \n",
    "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
    "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
    "s obviously the British number two, comfortably. 'So they can complain but the b\n",
    "est thing to do is use it in the right way and accept it for what it is, and try\n",
    " to use it as motivation whether they agree with it or not. He's British now so \n",
    "they've just got to deal with it. Murray stretches for a return after starting h\n",
    "is quarter final match slowly on the show court . Thiem held nothing back as he \n",
    "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
    "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
    "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
    ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
    "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
    "en as a positive for those guys using it to try to get better. He's a good playe\n",
    "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
    "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
    "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
    ": 'He's a good player with a very good serve. He's a legitimate top 100 player, \n",
    "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
    "our he wins matches, it's not like he turns up and always loses in the first rou\n",
    "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
    "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
    "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
    "ell. I would imagine he would keep moving up the rankings although I don't know \n",
    "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
    " seen him play loads, but when you serve as well as he does it helps. I would im\n",
    "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
    ".' It is understood the Lawn Tennis Association will give background support to \n",
    "his case regarding the Davis Cup but have made it clear that the onus is on him \n",
    "to lead the way. An official statement said: 'To have another player in the men'\n",
    "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
    "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
    "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
    "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
    "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
    "o life as he raced to a commanding lead in the second . The No 3 seed sent over \n",
    "a few glaring looks towards his team before winning the second set .\n",
    "\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcbc961e-6916-4f72-a0a8-ce0123e1ba6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Andy Murray defeated Dominic Thiem 3-6 6-4, 6-1 in Miami Open quarter final . The world No 4 is into the semi-finals of the Miami Open . Thiem lost in the second round of a Challenger event to\\xa0Aljaz Bedene . The 27-year-old believes his colleagues should use the assimilation of the world number 83 as motivation to better themselves .'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(LONG_BORING_TENNIS_ARTICLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e84f45-965e-4fec-9e18-2016ae1ffb4a",
   "metadata": {},
   "source": [
    "<a id=\"tg2\"> </a>\n",
    "### 1.6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdcd4cb8-25ec-4458-baf3-e7e92ca9901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c19ddd14d74d4181fc6d3f370808b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496e093e704c492c992f92a8c65bb6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7d9ea1dac4270a895cfec86c2b6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b08416c3b049c9b073061f2dafeb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96756941-7541-4da3-bbb2-0144993897d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main reason for India's loss is ills the country's own government, not the country's national leader, Nandini Babu's comments.\n",
      "\n",
      "Ramesh Prasad, an expert on Indian and U.S. political\n"
     ]
    }
   ],
   "source": [
    "output = text_generator(\"The main reason for India's loss is \", )\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f58ea1-457b-49d6-9f4e-52499a028f64",
   "metadata": {},
   "source": [
    "<a id=\"pfe\"> </a>\n",
    "### 1.7. Projection - Features Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e303aa81-9547-438e-896b-25eaf7db8b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased (https://huggingface.co/distilbert-base-cased)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc3f1a1ccd446ccb24dc20669e275e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181745ee50a94fbfb145daaa410ce444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217cbc95ca544828ac003cef85b3ceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606d3cc1b34e4d1e904a613d5c2cfbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e4aee412464e0f826e1008e49ccd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 12, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nlp_features = pipeline('feature-extraction')\n",
    "output = nlp_features('Hugging Face is a French company based in Paris')\n",
    "np.array(output).shape   # (Samples, Tokens, Vector Size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
