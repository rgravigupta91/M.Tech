{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfcb6c75-8af9-462c-8c30-9ca2cb62c3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and load the input file into a dataframe\n",
    "rawstrokeDF = spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\",',').load(\"/FileStore/tables/SparkMLlib/HeartStroke.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ada3050-642d-4968-b56a-8cde6f18a86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the schema and first few records\n",
    "rawstrokeDF.printSchema()\n",
    "rawstrokeDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04de888e-227a-4171-8e53-41fb645277db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note you can use display funciton only in Databricks which displays records in a tabular form\n",
    "display(rawstrokeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7da67df-58fd-4cdd-aa43-c1fd1cebd4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the count\n",
    "rawstrokeDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab1ab4c-a827-440d-9fa3-fdf418b8ac29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a summary description of the dataframe\n",
    "rawstrokeDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc852842-d8e3-4f54-9e10-0fc3f2ad6fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Two of the columns have nulls in some records, hence their count is lesser\n",
    " \n",
    "from pyspark.sql.functions import isnull, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4784aced-b3f5-49bf-acff-ad2048dd10e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter for the null records in one of the columns and get the count\n",
    "rawstrokeDF.filter(col('smoking_history').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfd2237-89e1-48e2-b2da-0fb6c72e5d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for the null records in the other column and get the count\n",
    "rawstrokeDF.filter(col('BMI').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3891a1b2-67bd-445a-9a9b-3e04c2da426d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get count of non-null records for the first column\n",
    "rawstrokeDF.filter(col('smoking_history').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73372378-0c7b-4fbd-a095-273758a8dd7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get count of non-null records for the second column\n",
    "rawstrokeDF.filter(col('BMI').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e292766b-58e2-49fe-89fc-eb8f24314e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of columns\n",
    "rawstrokeDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e104d3-f321-4be2-b312-e9ff198d76c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use list comprehension of Python language to get the columns and the respective count of nulls\n",
    "rawstrokeDF.select([count(when(isnull(c), c)).alias(c) for c in rawstrokeDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee04fcaa-fea9-46ca-8122-f931a8124ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the records which have nulls in any column \n",
    "rawstrokeDF = rawstrokeDF.na.drop()\n",
    "rawstrokeDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02c8167-c628-417d-991e-1b829ce2c1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use list comprehension again to get the columns and the respective count of nulls\n",
    "# The number of nulls for all columns should be 0 now as the records with nulls are dropped\n",
    "rawstrokeDF.select([count(when(isnull(c), c)).alias(c) for c in rawstrokeDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4450fbce-3e08-461b-be5e-2023638651ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a summary description of the dataframe\n",
    "rawstrokeDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f48bef-d6b9-4ed8-89b8-6ba036c5eb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display  few records to check\n",
    "display(rawstrokeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e392b45-e4cb-4742-a8b5-ee2b997555b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The variable values for any supervised ML algorithm has to be of type double.\n",
    "# Take a closer look at the data type of each column\n",
    "\n",
    "rawstrokeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4220581-575a-49db-ab8f-a01e6d308dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let us convert the columns \"diabetes\", \"hypertension\" and target varaible \"stroke\" data type into type double\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "strokeDF = rawstrokeDF.withColumn(\"diabetes\", col(\"diabetes\").cast(DoubleType())).withColumn(\"hypertension\", col(\"hypertension\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d3b563-7785-4c5e-80b9-140c2946d906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check the schema again. The data type for these two columns should be double now.\n",
    "strokeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8967be-41c4-4f08-b590-205f6aa5bf08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ### Transformations\n",
    "\n",
    "# #### Binarizer\n",
    "# Let us use divide the BMi into two groups: Obese and healthy. 1 represents 'obese' and 0 represents 'healthy' (If your BMI is 30.0 or higher, it falls within the obese range)\n",
    "# We will use the Binarizer transformer to create a new variable 'Body Type' (1- obese and 0- healthy) by binarizing the 'BMI' variable by setting the obesity threshold value 30.0. Binarization is used for thresholding numerical feature to binary feature (0 or 1)\n",
    "\n",
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(inputCol=\"BMI\", outputCol=\"BodyType\", threshold=30.0)\n",
    "binarizedDF = binarizer.transform(strokeDF)\n",
    "binarizedDF.select('BMI', 'BodyType').show(5,False)\n",
    "\n",
    "binarizedDF.printSchema()\n",
    "\n",
    "binarizedDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df277f3d-ca64-49d4-8d2c-4589d1ed73a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# From the above result we can see that the value of the target feature label is now converted to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d7fdef-6f1a-425f-9cb2-3dbbedea53c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #### Bucketizer\n",
    "# We now group the patients based on their age group. Here, we will use the Bucketizer transformer. Bucketizer is used for creating group of values of a continuous feature\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "# lets define the age age group splits\n",
    "splits = [0, 25.0, 50.0, 75.0, 100.0]\n",
    "bucketizer = Bucketizer(inputCol=\"age\", outputCol=\"ageGroup\", splits=splits)\n",
    "bucketizedDF = bucketizer.transform(binarizedDF)\n",
    "\n",
    "bucketizedDF.printSchema()\n",
    "bucketizedDF.select('age', 'ageGroup').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cfefd2-f261-48e5-9a31-4084349fc24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #### StringIndexer\n",
    "# There are three categorical variables in our dataset viz., 'gender', 'heart disease' and 'smoking history'. These variables cannot be directly passed to our ML algorithms. We will converet them into indexes and to do that we will use StringIndexer transformer. StringIndexer converts a string column to an index column. The most frequent label gets index 0\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexers = StringIndexer(inputCols= ['stroke','gender', 'heart_disease', 'smoking_history'], \n",
    "                         outputCols=['label', 'gender_indexed', 'heart_disease_indexed', 'smoking_history_indexed'])\n",
    "strindexedDF = indexers.fit(bucketizedDF).transform(bucketizedDF)\n",
    "\n",
    "strindexedDF.printSchema()\n",
    "strindexedDF.select('stroke', 'label', 'gender', 'gender_indexed', 'heart_disease', 'heart_disease_indexed', \n",
    "                    'smoking_history', 'smoking_history_indexed').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcfa2cd-f999-48ed-839b-bc630ff43c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### New Stage\n",
    "\n",
    "# ### VectorAssembler\n",
    "# MLlib expects all features to be contained within a single column. VectorAssembler combines multiple columns and gives single column as output\n",
    "\n",
    "# Import VectorAssembler from pyspark.ml.feature package\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a list of all the variables that are required in features vector\n",
    "# These features are then further used for training model\n",
    "\n",
    "# features_col = [\"age\", \"diabetes\", \"hypertension\", \"BMI\", \"BodyType\", \"ageGroup\", \"gender_indexed\",\"heart_disease_indexed\",\"smoking_history_indexed\"]\n",
    "\n",
    "features_col = [\"diabetes\", \"hypertension\", \"BodyType\", \"ageGroup\", \"gender_indexed\",\"heart_disease_indexed\",\"smoking_history_indexed\"]\n",
    "\n",
    "# Create the VectorAssembler object and use it to transform the dataframe to add a vector type column features\n",
    "\n",
    "assembler = VectorAssembler(inputCols= features_col, outputCol= \"features\")\n",
    "assembledDF = assembler.transform(strindexedDF)\n",
    "\n",
    "assembledDF.printSchema()\n",
    "assembledDF.select(\"features\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04e9c34-fcae-433f-b4a9-ab2f9b837492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As we see for some records, the vector column displays all the values. This is called dense vector.\n",
    "# For example [0.0,1.0,0.0,3.0,1.0,1.0,2.0]\n",
    "# For some records, the vector column displays the size of the vector, then the list of non-zero value positions and lastly the non-zero values. This is nown as sparse vector.\n",
    "# For example (7,[3,5],[3.0,1.0]) \n",
    "# In the above vector size is 7 (0 to 6). In these 7 positions position 3, and 5 have non-zero values. These are 3.0 and 1.0\n",
    "# This in dense format would be [0.0,0.0,0.0,3.0,0.0,1.0,0.0] which occupies more space than sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac563ac-9d52-4af1-b485-174bf5d96396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now all required features are vectorized.\n",
    "\n",
    "# ## Spark ML Decision Tree Classification\n",
    "# We will now train the ML model with the data that we have transformed so far. We will build classification model since, given the data, we need to determine if a person will get a stroke or not.\n",
    "\n",
    "# ### Train-Test Split\n",
    "# We split the output of  data into training and test sets (30% held out for testing)\n",
    "# Note: This train-test split of for logistic regression\n",
    "\n",
    "# We spilt the data into 70-30 set\n",
    "# Training Set - 70% obesevations\n",
    "# Testing Set - 30% observations\n",
    "trainDF, testDF =  assembledDF.randomSplit([0.7,0.3], seed = 2020)\n",
    "\n",
    "# print the count of observations in each set\n",
    "print(\"Observations in training set = \", trainDF.count())\n",
    "print(\"Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ae5d5c-f7dd-4847-bf96-3197ae2ddd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ### Supervised Learning - Decision Tree Classification \n",
    "\n",
    "# import the DecisionTree function from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create the DecisionTreeClassifier object 'dtc' by setting the required parameters\n",
    "# We will pass the VectorIndexed columns as featureCol and maxDepth which is a stopping criterion to Decision Tree Classifier.\n",
    "\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\",maxDepth= 10)\n",
    "\n",
    "# Fit the DecisionTreeClassifier object on the training data to produce the model\n",
    "\n",
    "dtcmodel = dtc.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fd5532-d723-40fd-a043-effab08d1cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cc8e53-df6b-4a03-a2b3-673e64d9fa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(dtcmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6870e38-05b3-4bbf-abeb-551178a9226a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This DecisionTreeClassificationModel can be used as a transformer to perform prediction on the testing data\n",
    "\n",
    "dtcpredictionDF = dtcmodel.transform(testDF)\n",
    "\n",
    "dtcpredictionDF.printSchema()\n",
    "\n",
    "dtcpredictionDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58670f30-9b4a-4a0f-9601-66eabfa00aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# rawPrediction is the raw output of the classifier (array with length equal to the number of classes)\n",
    "# probability is the result of applying the function to rawPrediction (array of length equal to that of rawPrediction)\n",
    "# prediction is the argument where the array probability takes its maximum value, and it gives the most probable label (single number)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6464dbe5-6474-43c8-a56d-7ed2cc5748d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ##### Model Evaluation\n",
    "\n",
    "# import MulticlassClassificationEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Build the MulticlassClassificationEvaluator object 'evaluator'\n",
    "multievaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(dtcpredictionDF, {multievaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', multievaluator.evaluate(dtcpredictionDF))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(dtcpredictionDF, {multievaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(dtcpredictionDF, {multievaluator.metricName: \"weightedRecall\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b255dbf8-484b-49c1-a39e-79ea2dc51afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dtcmodel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830b2016-ae7b-43ac-8176-39a36fbdca7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ### Model Persistence\n",
    "# Model persistence means saving your model to a disk. After you finalize your model for prediction depending upon the performance, you need to save the model to the disk. Let's say, you finalize 'dtpipelinemodel' to be used for in production environment i.e. in your application. We use the following code to save it.\n",
    "\n",
    "# ##### Saving the model\n",
    "\n",
    "# use save() method to save the model\n",
    "# write().overwrite() is usually used when you want to replace the older model with a new one\n",
    "# It might happen that you wish to retrain your model and save it at the same the place\n",
    "\n",
    "dtcmodel.write().overwrite().save(\"/FileStore/tables/SparkMLlib/dtcmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc6d9fb-efb5-4646-bbf8-b1376b87c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/SparkMLlib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38ab630-f333-4190-8a16-87dd59542e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/SparkMLlib/dtcmodel\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySparkMLlibDTC",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
