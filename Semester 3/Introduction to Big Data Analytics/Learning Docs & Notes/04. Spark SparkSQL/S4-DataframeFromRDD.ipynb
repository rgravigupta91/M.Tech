{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/07 11:51:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "# initializing spark session\n",
    "spark = SparkSession(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02b0fb03-7e15-46eb-98dc-eba26d9b3d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records including header :  950\n",
      "Total records including header :  949\n",
      "+------------------+----------+----------+\n",
      "|Region            |Country   |ItemType  |\n",
      "+------------------+----------+----------+\n",
      "|Sub-Saharan Africa|Burundi   |Vegetables|\n",
      "|Europe            |Ukraine   |Cosmetics |\n",
      "|Europe            |Croatia   |Beverages |\n",
      "|Sub-Saharan Africa|Madagascar|Fruits    |\n",
      "|Asia              |Malaysia  |Snacks    |\n",
      "+------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creditcardRDD = sc.textFile(\"SalesTraining.csv\")    # Reading the file from DBFS location\n",
    "\n",
    "# display record count - before removing the header record\n",
    "print(\"Total records including header : \", creditcardRDD.count())\n",
    "\n",
    "header = creditcardRDD.first() # extract the header record from the RDD\n",
    "dataRDD = creditcardRDD.filter(lambda row: row != header)   # use 'filter' (transformation) to remove the header\n",
    "\n",
    "# display record count - after removing the header record\n",
    "print(\"Total records including header : \", dataRDD.count())\n",
    "\n",
    "# split the record by ',' and select only the first 3 columns - Region, Country and Item Type\n",
    "tupleRDD = dataRDD.map(lambda each_row: each_row.split(',')).map(lambda each_row: (each_row[0], each_row[1], each_row[2])) \n",
    "\n",
    "# convert the RDD to a DataFrame, display first 5 rows with no truncation of column data\n",
    "columns = [\"Region\", \"Country\", \"ItemType\"]\n",
    "\n",
    "# convert the RDD to DataFrame. Show() is as action, which shows 1st 5 records with record truncate=False\n",
    "tupleRDD.toDF(columns).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45ef659c-3d16-41a1-9a74-095da5e33a00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- ItemType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the tuple RDD to DataFrame with header record\n",
    "creditcardDF = tupleRDD.toDF(columns)\n",
    "\n",
    "# display the schema\n",
    "creditcardDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0050988-16db-4c0a-80e0-6f47fa643a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Region='Sub-Saharan Africa', Country='Burundi', ItemType='Vegetables'),\n",
       " Row(Region='Europe', Country='Ukraine', ItemType='Cosmetics'),\n",
       " Row(Region='Europe', Country='Croatia', ItemType='Beverages')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the columns\n",
    "creditcardDF.columns\n",
    "\n",
    "# display the RDD data - top 3 records\n",
    "creditcardDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da5c4e85-f83c-42c1-9da9-b38225fe6bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %scala code \n",
    "# val rdd=sc.parallelize(0 to 10,1)\n",
    "# val last=rdd.sortBy(x=>{x},false,1).first()\n",
    "# println(last)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dataframeFromRDD",
   "notebookOrigID": 1387049276820100,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
