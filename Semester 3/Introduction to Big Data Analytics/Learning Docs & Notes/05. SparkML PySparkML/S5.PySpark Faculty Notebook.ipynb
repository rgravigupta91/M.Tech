{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79a2b2cc-ac7f-42cb-82fa-24623e044b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Faculty Noteebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce5ea9ad-28be-481a-9585-bb13be1d13d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/07 17:01:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/07 17:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/07 17:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/07 17:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/08/07 17:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/08/07 17:01:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count, col,avg\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    " \n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71c02f53-3926-4957-b894-d52b4826c359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_location = \"HeartStroke.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = True\n",
    "first_row_is_header = True\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "rawstrokeDF = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "333622c0-50b5-48e5-ba8c-5d9ac63ede03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "|gender|age |diabetes|hypertension|stroke|heart_disease|smoking_history|BMI  |\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "|Female|80.0|0       |0           |No    |Yes          |never          |25.19|\n",
      "|Female|54.0|0       |0           |No    |No           |null           |null |\n",
      "|Male  |28.0|0       |0           |No    |No           |never          |null |\n",
      "|Female|36.0|0       |0           |No    |No           |current        |23.45|\n",
      "|Male  |76.0|0       |1           |No    |Yes          |current        |20.14|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawstrokeDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "561ff791-7802-4c4e-bfef-ca4a2643a238",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check missing values\n",
    "(For this exercise we will drop all the na values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cffa5a3-3abd-4950-9eaa-942dc2d48215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+------------+------+-------------+---------------+-----+\n",
      "|gender|age|diabetes|hypertension|stroke|heart_disease|smoking_history|  BMI|\n",
      "+------+---+--------+------------+------+-------------+---------------+-----+\n",
      "|     0|  0|       0|           0|     0|            0|          35816|25444|\n",
      "+------+---+--------+------------+------+-------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "rawstrokeDF.select([count(when(isnull(c), c)).alias(c) for c in rawstrokeDF.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fec31532-72bd-42cc-96b5-6c0ba6870ce9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The columns 'smoking history' and 'BMI' has missing values. Let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "348fafeb-e34e-41f3-b5d0-0909e54e00ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use df_name.na.drop() to drop all the null values from the dataframe\n",
    "rawstrokeDF = rawstrokeDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44d9ee7a-09b6-4193-8d90-619746f92be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+------------+------+-------------+---------------+---+\n",
      "|gender|age|diabetes|hypertension|stroke|heart_disease|smoking_history|BMI|\n",
      "+------+---+--------+------------+------+-------------+---------------+---+\n",
      "|     0|  0|       0|           0|     0|            0|              0|  0|\n",
      "+------+---+--------+------------+------+-------------+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the null value still exist\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "rawstrokeDF.select([count(when(isnull(c), c)).alias(c) for c in rawstrokeDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84717b60-68dc-4f31-94c2-11c6c52be3b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "|gender|age |diabetes|hypertension|stroke|heart_disease|smoking_history|BMI  |\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "|Female|80.0|0       |0           |No    |Yes          |never          |25.19|\n",
      "|Female|36.0|0       |0           |No    |No           |current        |23.45|\n",
      "|Male  |76.0|0       |1           |No    |Yes          |current        |20.14|\n",
      "|Female|44.0|1       |0           |No    |No           |never          |19.31|\n",
      "|Male  |42.0|0       |0           |No    |No           |never          |33.64|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawstrokeDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b4b0fa8-e3d5-4b05-a639-1b9b9278cff3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check and confirm the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b0f6903-1879-4640-b78a-aaea359bcddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- heart_disease: string (nullable = true)\n",
      " |-- smoking_history: string (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawstrokeDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f556b9f0-da56-402d-b810-a16a47cdf11b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The variable values for any supervised ML algorithm has to be of type double. Let us convert the columns \"diabetes\", \"hypertension\" and target varaible \"stroke\" data type into type double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b71dc4ed-8fa4-4080-b0cf-ca37c6e99a66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- hypertension: double (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- heart_disease: string (nullable = true)\n",
      " |-- smoking_history: string (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "intcols = [\"diabetes\", \"hypertension\"]\n",
    "\n",
    "for col_name in intcols:\n",
    "    strokeDF = rawstrokeDF.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# We will also rename the column name 'stroke' as 'label' for \n",
    "\n",
    "strokeDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "763adba8-53fb-4ff1-a1d2-9489701434b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "846da4ad-4b34-479c-bf2e-40bbfad20890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "27525112-c958-46fb-8662-256672530ff6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets use divide the BMi into two groups: Obese and healthy. 1 represents 'obese' and 0 represents 'healthy' (If your BMI is 30.0 or higher, it falls within the obese range)\n",
    "We will use the Binarizer transformer to create a new variable 'Body Type' (1- obese and 0- healthy) by binarizing the 'BMI' variable by setting the obesity threshold value 30.0. Binarization is used for thresholding numerical feature to binary feature (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "990dff90-725f-4049-8c63-486f6369475b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|BMI  |BodyType|\n",
      "+-----+--------+\n",
      "|25.19|0.0     |\n",
      "|23.45|0.0     |\n",
      "|20.14|0.0     |\n",
      "|19.31|0.0     |\n",
      "|33.64|1.0     |\n",
      "+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(inputCol=\"BMI\", outputCol=\"BodyType\", threshold=30.0)\n",
    "binarizedDF = binarizer.transform(strokeDF)\n",
    "binarizedDF.select('BMI', 'BodyType').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c1a5da8-81cd-4ed7-9736-8343ef2a04aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+\n",
      "|gender| age|diabetes|hypertension|stroke|heart_disease|smoking_history|  BMI|BodyType|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+\n",
      "|Female|80.0|       0|         0.0|    No|          Yes|          never|25.19|     0.0|\n",
      "|Female|36.0|       0|         0.0|    No|           No|        current|23.45|     0.0|\n",
      "|  Male|76.0|       0|         1.0|    No|          Yes|        current|20.14|     0.0|\n",
      "|Female|44.0|       1|         0.0|    No|           No|          never|19.31|     0.0|\n",
      "|  Male|42.0|       0|         0.0|    No|           No|          never|33.64|     1.0|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizedDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "311aac72-8a92-48c3-ad00-a22c083e8a05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the above result we can see that the value of the target feature label is now converted to binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06f94d2f-3171-44fe-bb46-b47528ca487b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bucketizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0127f53-09ff-442d-bec8-a1d55d2a0951",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now group the patients based on their age group. Here, we will use the Bucketizer transformer. Bucketizer is used for creating group of values of a continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2754e9f5-d96e-4d6e-ade4-534622a24958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|age |ageGroup|\n",
      "+----+--------+\n",
      "|80.0|3.0     |\n",
      "|36.0|1.0     |\n",
      "|76.0|3.0     |\n",
      "|44.0|1.0     |\n",
      "|42.0|1.0     |\n",
      "|54.0|2.0     |\n",
      "|78.0|3.0     |\n",
      "|67.0|2.0     |\n",
      "|15.0|0.0     |\n",
      "|42.0|1.0     |\n",
      "+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "# lets define the age age group splits\n",
    "splits = [0, 25.0, 50.0, 75.0, 100.0]\n",
    "bucketizer = Bucketizer(inputCol=\"age\", outputCol=\"ageGroup\", splits=splits)\n",
    "bucketizedDF = bucketizer.transform(binarizedDF)\n",
    "bucketizedDF.select('age', 'ageGroup').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "787e3539-9540-4b78-a31f-fcc09d13e91c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'diabetes',\n",
       " 'hypertension',\n",
       " 'stroke',\n",
       " 'heart_disease',\n",
       " 'smoking_history',\n",
       " 'BMI',\n",
       " 'BodyType',\n",
       " 'ageGroup']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucketizedDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf0edd88-3ffc-4d5a-93dc-a7f15f3d9914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|age |ageGroup|\n",
      "+----+--------+\n",
      "|80.0|3.0     |\n",
      "|36.0|1.0     |\n",
      "|76.0|3.0     |\n",
      "|44.0|1.0     |\n",
      "|42.0|1.0     |\n",
      "|54.0|2.0     |\n",
      "|78.0|3.0     |\n",
      "|67.0|2.0     |\n",
      "|15.0|0.0     |\n",
      "|42.0|1.0     |\n",
      "+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketizedDF.select('age', 'ageGroup').show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3890748-994d-4932-9128-0c10cf3823f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c86ffaa-a614-43ea-a78f-cbff45f60050",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are three categorical variables in our dataset viz., 'gender', 'heart disease' and 'smoking history'. These variables cannot be directly passed to our ML algorithms. We will converet them into indexes and to do that we will use StringIndexer transformer. StringIndexer converts a string column to an index column. The most frequent label gets index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdb6b4f7-77ae-4783-a9da-ed74cac1fe52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+--------------+-------------+---------------------+---------------+-----------------------+\n",
      "|stroke|label|gender|gender_indexed|heart_disease|heart_disease_indexed|smoking_history|smoking_history_indexed|\n",
      "+------+-----+------+--------------+-------------+---------------------+---------------+-----------------------+\n",
      "|No    |0.0  |Female|0.0           |Yes          |1.0                  |never          |0.0                    |\n",
      "|No    |0.0  |Female|0.0           |No           |0.0                  |current        |2.0                    |\n",
      "|No    |0.0  |Male  |1.0           |Yes          |1.0                  |current        |2.0                    |\n",
      "|No    |0.0  |Female|0.0           |No           |0.0                  |never          |0.0                    |\n",
      "|No    |0.0  |Male  |1.0           |No           |0.0                  |never          |0.0                    |\n",
      "+------+-----+------+--------------+-------------+---------------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexers = StringIndexer(inputCols= ['stroke','gender', 'heart_disease', 'smoking_history'], \n",
    "                         outputCols=['label', 'gender_indexed', 'heart_disease_indexed', 'smoking_history_indexed'])\n",
    "strindexedDF = indexers.fit(bucketizedDF).transform(bucketizedDF)\n",
    "strindexedDF.select('stroke', 'label', 'gender', 'gender_indexed', 'heart_disease', 'heart_disease_indexed', \n",
    "                    'smoking_history', 'smoking_history_indexed').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "705bdcd4-8da4-486d-b5b5-735859428148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the above output you can observe that the categorical columns has got converted to their respective indices columns. The most frequent label gets index 0 and so on ordered by label frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a91d58e8-0827-407d-a6b9-4ecdfa7abd39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bc5dc5c-0021-42be-b1f6-6a204431521f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------------+----------------+-----------------------+------------------+\n",
      "|gender_indexed|genderVec    |heart_disease_indexed|heart_diseaseVec|smoking_history_indexed|smoking_historyVec|\n",
      "+--------------+-------------+---------------------+----------------+-----------------------+------------------+\n",
      "|0.0           |(2,[0],[1.0])|1.0                  |(1,[],[])       |0.0                    |(4,[0],[1.0])     |\n",
      "|0.0           |(2,[0],[1.0])|0.0                  |(1,[0],[1.0])   |2.0                    |(4,[2],[1.0])     |\n",
      "|1.0           |(2,[1],[1.0])|1.0                  |(1,[],[])       |2.0                    |(4,[2],[1.0])     |\n",
      "|0.0           |(2,[0],[1.0])|0.0                  |(1,[0],[1.0])   |0.0                    |(4,[0],[1.0])     |\n",
      "|1.0           |(2,[1],[1.0])|0.0                  |(1,[0],[1.0])   |0.0                    |(4,[0],[1.0])     |\n",
      "+--------------+-------------+---------------------+----------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols= [\"gender_indexed\", 'heart_disease_indexed', 'smoking_history_indexed'], \n",
    "                         outputCols=[\"genderVec\", 'heart_diseaseVec', 'smoking_historyVec'])\n",
    "encodedDF = encoder.fit(strindexedDF).transform(strindexedDF)\n",
    "encodedDF.select('gender_indexed', 'genderVec', 'heart_disease_indexed', 'heart_diseaseVec', \n",
    "                    'smoking_history_indexed', 'smoking_historyVec',).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2aa7118-462a-4332-8f88-b14c1517d272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+--------+-----+--------------+---------------------+-----------------------+-------------+----------------+------------------+\n",
      "|gender| age|diabetes|hypertension|stroke|heart_disease|smoking_history|  BMI|BodyType|ageGroup|label|gender_indexed|heart_disease_indexed|smoking_history_indexed|    genderVec|heart_diseaseVec|smoking_historyVec|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+--------+-----+--------------+---------------------+-----------------------+-------------+----------------+------------------+\n",
      "|Female|80.0|       0|         0.0|    No|          Yes|          never|25.19|     0.0|     3.0|  0.0|           0.0|                  1.0|                    0.0|(2,[0],[1.0])|       (1,[],[])|     (4,[0],[1.0])|\n",
      "|Female|36.0|       0|         0.0|    No|           No|        current|23.45|     0.0|     1.0|  0.0|           0.0|                  0.0|                    2.0|(2,[0],[1.0])|   (1,[0],[1.0])|     (4,[2],[1.0])|\n",
      "|  Male|76.0|       0|         1.0|    No|          Yes|        current|20.14|     0.0|     3.0|  0.0|           1.0|                  1.0|                    2.0|(2,[1],[1.0])|       (1,[],[])|     (4,[2],[1.0])|\n",
      "|Female|44.0|       1|         0.0|    No|           No|          never|19.31|     0.0|     1.0|  0.0|           0.0|                  0.0|                    0.0|(2,[0],[1.0])|   (1,[0],[1.0])|     (4,[0],[1.0])|\n",
      "|  Male|42.0|       0|         0.0|    No|           No|          never|33.64|     1.0|     1.0|  0.0|           1.0|                  0.0|                    0.0|(2,[1],[1.0])|   (1,[0],[1.0])|     (4,[0],[1.0])|\n",
      "+------+----+--------+------------+------+-------------+---------------+-----+--------+--------+-----+--------------+---------------------+-----------------------+-------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodedDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7028a0a5-68d9-4923-a76a-15907b8d048b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### VectorAssembler\n",
    "MLlib expects all features to be contained within a single column. VectorAssembler combines multiple columns and gives single column as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abbff48a-2d03-48a2-88e6-64301d21a7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+\n",
      "|features                                             |\n",
      "+-----------------------------------------------------+\n",
      "|(13,[0,3,5,6,9],[80.0,25.19,3.0,1.0,1.0])            |\n",
      "|(13,[0,3,5,6,8,11],[36.0,23.45,1.0,1.0,1.0,1.0])     |\n",
      "|(13,[0,2,3,5,7,11],[76.0,1.0,20.14,3.0,1.0,1.0])     |\n",
      "|(13,[0,1,3,5,6,8,9],[44.0,1.0,19.31,1.0,1.0,1.0,1.0])|\n",
      "|(13,[0,3,4,5,7,8,9],[42.0,33.64,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import VectorAssembler from pyspark.ml.feature package\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Create a list of all the variables that you want to create feature vectors\n",
    "# These features are then further used for training model\n",
    "features_col = [\"age\", \"diabetes\", \"hypertension\", \"BMI\", \"BodyType\", \"ageGroup\", \n",
    "                \"genderVec\",\"heart_diseaseVec\",\"smoking_historyVec\"]\n",
    "# Create the VectorAssembler object\n",
    "assembler = VectorAssembler(inputCols= features_col, outputCol= \"features\")\n",
    "assembledDF = assembler.transform(encodedDF)\n",
    "assembledDF.select(\"features\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77ffc422-7e1f-4272-8464-cbccb9dc569c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that all the features are vectorized. Let us convert the target variable 'stroke' (integer) in to a label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c8f0c284-0744-46cf-9c68-781183989ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e229fd9-8603-4b0e-bc0b-720fbab8aee3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "All the features are now in one single feature vector. If you notice, the feature column contains sparse vector. In order to perform scaling on the data, we must convert the sparse vector to dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "484173a9-f793-48ef-96b3-f56e4c7d97ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'diabetes',\n",
       " 'hypertension',\n",
       " 'stroke',\n",
       " 'heart_disease',\n",
       " 'smoking_history',\n",
       " 'BMI',\n",
       " 'BodyType',\n",
       " 'ageGroup',\n",
       " 'label',\n",
       " 'gender_indexed',\n",
       " 'heart_disease_indexed',\n",
       " 'smoking_history_indexed',\n",
       " 'genderVec',\n",
       " 'heart_diseaseVec',\n",
       " 'smoking_historyVec',\n",
       " 'features']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembledDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5bc4c0eb-c13f-4f38-a895-c47b87656c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### VectorIndexer\n",
    "VectorIndexer automatically identifies the categorical features from the feature vector (output from VectorAssembler). It then indexes categorical features inside of a Vector\n",
    "It is the vectorized version of StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d9755f3-f9ae-47f6-8ac6-4e91d5e16f65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+\n",
      "|features                                |indexed_features                        |\n",
      "+----------------------------------------+----------------------------------------+\n",
      "|(9,[0,3,5,7],[80.0,25.19,3.0,1.0])      |(9,[0,3,5,7],[80.0,25.19,3.0,1.0])      |\n",
      "|(9,[0,3,5,8],[36.0,23.45,1.0,2.0])      |(9,[0,3,5,8],[36.0,23.45,1.0,2.0])      |\n",
      "|[76.0,0.0,1.0,20.14,0.0,3.0,1.0,1.0,2.0]|[76.0,0.0,1.0,20.14,0.0,3.0,1.0,1.0,2.0]|\n",
      "|(9,[0,1,3,5],[44.0,1.0,19.31,1.0])      |(9,[0,1,3,5],[44.0,1.0,19.31,1.0])      |\n",
      "|[42.0,0.0,0.0,33.64,1.0,1.0,1.0,0.0,0.0]|[42.0,0.0,0.0,33.64,1.0,1.0,1.0,0.0,0.0]|\n",
      "+----------------------------------------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import VectorIndexer from pyspark.ml.feature package\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "# Create a list of all the raw features\n",
    "# VectorIndexer will automatically identify the categorical columns and index them\n",
    "featurecol = ['age', 'diabetes','hypertension', 'BMI','BodyType','ageGroup', \n",
    "              \"gender_indexed\", 'heart_disease_indexed', 'smoking_history_indexed']\n",
    "\n",
    "# Create the VectorAssembler object\n",
    "assembler = VectorAssembler(inputCols= featurecol, outputCol= \"features\")\n",
    "assembledDF = assembler.transform(strindexedDF)\n",
    "\n",
    "# Create the VectorIndexer object. It only take feature column\n",
    "vecindexer = VectorIndexer(inputCol= \"features\", outputCol= \"indexed_features\")\n",
    "# Fit the vectorindexer object on the output of the vectorassembler data and transform\n",
    "vecindexedDF = vecindexer.fit(assembledDF).transform(assembledDF)\n",
    "vecindexedDF.select(\"features\", \"indexed_features\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ddfc40ba-20fa-4bce-ae5b-c6400bab284b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "VectorIndexer let us skip the one hot encoding stage for encoding the categorical features. As discussed earlier, we should not use one hot encoding on categorical variables for algorithms like decision tree and tree ensembles. VectorIndexer are chosen over OneHotEncoderEstimator in such scenario which allows these algorithms to treat categorical features appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "703f8ea7-8e39-4f06-bc5e-8c72a513eb79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### StandardScaler\n",
    "StandardScaler scales each value in the feature vector such that the mean is 0 and the standard deviation is 1\n",
    "<br>It takes parameters:\n",
    "<br>withStd: True by default. Scales the data to unit standard deviation\n",
    "<br>withMean: False by default. Centers the data with mean before scaling\n",
    "\n",
    "**If you notice the output of vectorassembler data is a sparse vector. This need to be converted into a dense vector befor applying scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98e91727-3011-4d06-828f-3829c78c68d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+\n",
      "|features                                |features_array                          |\n",
      "+----------------------------------------+----------------------------------------+\n",
      "|(9,[0,3,5,7],[80.0,25.19,3.0,1.0])      |[80.0,0.0,0.0,25.19,0.0,3.0,0.0,1.0,0.0]|\n",
      "|(9,[0,3,5,8],[36.0,23.45,1.0,2.0])      |[36.0,0.0,0.0,23.45,0.0,1.0,0.0,0.0,2.0]|\n",
      "|[76.0,0.0,1.0,20.14,0.0,3.0,1.0,1.0,2.0]|[76.0,0.0,1.0,20.14,0.0,3.0,1.0,1.0,2.0]|\n",
      "|(9,[0,1,3,5],[44.0,1.0,19.31,1.0])      |[44.0,1.0,0.0,19.31,0.0,1.0,0.0,0.0,0.0]|\n",
      "|[42.0,0.0,0.0,33.64,1.0,1.0,1.0,0.0,0.0]|[42.0,0.0,0.0,33.64,1.0,1.0,1.0,0.0,0.0]|\n",
      "+----------------------------------------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Define a udf that converts sparse vector into dense vector\n",
    "# You cannot create your own custom function and run that against the data directly. \n",
    "# In Spark, You have to register the function first using udf function\n",
    "sparseToDense = F.udf(lambda v : Vectors.dense(v), VectorUDT())\n",
    "\n",
    "# We then call the function here passing the column name on which the function has to be applied\n",
    "densefeatureDF = assembledDF.withColumn('features_array', sparseToDense('features'))\n",
    "\n",
    "densefeatureDF.select(\"features\", \"features_array\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43f831f4-458c-403f-9f30-7e41631a766c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- hypertension: double (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- heart_disease: string (nullable = true)\n",
      " |-- smoking_history: string (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- BodyType: double (nullable = true)\n",
      " |-- ageGroup: double (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- gender_indexed: double (nullable = false)\n",
      " |-- heart_disease_indexed: double (nullable = false)\n",
      " |-- smoking_history_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_array: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "densefeatureDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "16de23c4-4e14-4c7c-bf2b-c1e44b1d181c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaledfeatures                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[4.087032486459775,0.0,0.0,3.4969597048301693,0.0,3.465141331509375,0.0,4.588062443054963,0.0]                                           |\n",
      "|[1.839164618906899,0.0,0.0,3.255407109101527,0.0,1.155047110503125,0.0,0.0,1.5657422910924597]                                           |\n",
      "|[3.882680862136787,0.0,3.266884631584384,2.795901883893593,0.0,3.465141331509375,2.0514943144150433,4.588062443054963,1.5657422910924597]|\n",
      "|[2.2478678675528765,3.122394188242338,0.0,2.680678519264413,0.0,1.155047110503125,0.0,0.0,0.0]                                           |\n",
      "|[2.145692055391382,0.0,0.0,4.670016850753747,2.080052044325287,1.155047110503125,2.0514943144150433,0.0,0.0]                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import StandardScaler from pyspark.ml.feature package\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Create the StandardScaler object. It only take feature column (dense vector)\n",
    "stdscaler = StandardScaler(inputCol= \"features_array\", outputCol= \"scaledfeatures\")\n",
    "\n",
    "# Fit the StandardScaler object on the output of the dense vector data and transform\n",
    "stdscaledDF = stdscaler.fit(densefeatureDF).transform(densefeatureDF)\n",
    "stdscaledDF.select(\"scaledfeatures\" ).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72aff8c0-26dd-4a30-ad56-7772c656bc6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the above output we can see that the features are scaled to unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82495ab0-fed5-4934-ac3b-a54ecd67ca38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- hypertension: double (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- heart_disease: string (nullable = true)\n",
      " |-- smoking_history: string (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- BodyType: double (nullable = true)\n",
      " |-- ageGroup: double (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- gender_indexed: double (nullable = false)\n",
      " |-- heart_disease_indexed: double (nullable = false)\n",
      " |-- smoking_history_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_array: vector (nullable = true)\n",
      " |-- scaledfeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdscaledDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6297b568-d4ef-4c72-8ada-1082d3853326",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MinMaxScaler\n",
    "For MinMaxScaler you dont need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a30581b-fe6a-4f70-8531-f1342c44cf0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|mmxscaledfeatures                                                                  |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|(9,[0,3,5,7],[0.9999999999999999,0.18485441644237832,1.0,1.0])                     |\n",
      "|(9,[0,3,5,8],[0.44889779559118237,0.1635674088573526,0.3333333333333333,0.5])      |\n",
      "|[0.9498997995991983,0.0,1.0,0.12307315879618304,0.0,1.0,0.5,1.0,0.5]               |\n",
      "|(9,[0,1,3,5],[0.5490981963927856,1.0,0.11291901149987765,0.3333333333333333])      |\n",
      "|[0.5240480961923848,0.0,0.0,0.28823097626620997,1.0,0.3333333333333333,0.5,0.0,0.0]|\n",
      "+-----------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import MinMaxScaler from pyspark.ml.feature package\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Create the MinMaxScaler object. It only take feature column (dense vector)\n",
    "mmxscaler = MinMaxScaler(inputCol= \"features_array\", outputCol= \"mmxscaledfeatures\", )\n",
    "\n",
    "# Fit the MinMaxScaler object on the output of the dense vector data and transform\n",
    "mmxscaledDF = mmxscaler.fit(densefeatureDF).transform(densefeatureDF)\n",
    "mmxscaledDF.select(\"mmxscaledfeatures\" ).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2030c0cc-5120-451a-af3e-96c71dbe918e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5184646-7e67-4950-9d8f-fbf5685e546c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f78aa3b6-d2d8-47d9-a450-48da1e3b9a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|normscaledfeatures                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.9531555641945476,0.0,0.0,0.3001248582757582,0.0,0.03574333365729553,0.0,0.011914444552431845,0.0]                                                   |\n",
      "|[0.8367789176331486,0.0,0.0,0.5450684894027037,0.0,0.023243858823143018,0.0,0.0,0.046487717646286036]                                                  |\n",
      "|[0.9653863060221477,0.0,0.012702451395028258,0.2558273710958691,0.0,0.03810735418508478,0.012702451395028258,0.012702451395028258,0.025404902790056517]|\n",
      "|[0.9153018623338187,0.020802315053041334,0.0,0.4016927036742281,0.0,0.020802315053041334,0.0,0.0,0.0]                                                  |\n",
      "|[0.7801020707748294,0.0,0.0,0.6248246109729825,0.01857385882797213,0.01857385882797213,0.01857385882797213,0.0,0.0]                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import norm from pyspark.ml.feature package\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Create the norm object. It only take feature column (dense vector)\n",
    "normscaler = Normalizer(inputCol= \"features_array\", outputCol= \"normscaledfeatures\")\n",
    "\n",
    "# Fit the norm object on the output of the dense vector data and transform\n",
    "normscaledDF = normscaler.transform(densefeatureDF)\n",
    "normscaledDF.select(\"normscaledfeatures\" ).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "264f8e53-44bc-45a6-9688-2333889d93b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- diabetes: integer (nullable = true)\n",
      " |-- hypertension: double (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- heart_disease: string (nullable = true)\n",
      " |-- smoking_history: string (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- BodyType: double (nullable = true)\n",
      " |-- ageGroup: double (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- gender_indexed: double (nullable = false)\n",
      " |-- heart_disease_indexed: double (nullable = false)\n",
      " |-- smoking_history_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_array: vector (nullable = true)\n",
      " |-- normscaledfeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normscaledDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cee2980c-e1d3-4847-818c-adf4969068c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### You can use any of the above scaled data for training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4768949d-4a84-4a7d-a67c-f401900d70ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark ML Algorithms\n",
    "We will now train the ML models with the data that we have worked upon so far. We will build classification model since, given the data, we need to determine if a person will get a stroke or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1b7d58c-0d75-4bdb-922c-4c8564eef776",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train-Test Split\n",
    "We split the output of  data into training and test sets (30% held out for testing)\n",
    "Note: This train-test split of for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "733e90d5-e6f8-4cb4-8105-f2c393205af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set =  36680\n",
      "Observations in testing set =  15495\n"
     ]
    }
   ],
   "source": [
    "# We spilt the data into 70-30 set\n",
    "# Training Set - 70% obesevations\n",
    "# Testing Set - 30% observations\n",
    "trainDF, testDF =  assembledDF.randomSplit([0.7,0.3], seed = 2020)\n",
    "\n",
    "# print the count of observations in each set\n",
    "print(\"Observations in training set = \", trainDF.count())\n",
    "print(\"Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b4e419c-f104-4950-9799-9bc83fbbd33c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Supervised Learning - Classification \n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3343ab93-7cd6-4785-be0b-e85b3404524c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/07 17:01:54 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "22/08/07 17:01:54 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/08/07 17:01:54 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "|label|rawPrediction                         |probability                              |prediction|\n",
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the LogisticRegression function from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Build the LogisticRegression object 'lr' by setting the required parameters\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\",maxIter= 10,regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# fit the LogisticRegression object on the training data\n",
    "lrmodel = lr.fit(trainDF)\n",
    "\n",
    "#This LogisticRegressionModel can be used as a transformer to perform prediction on the testing data\n",
    "predictonDF = lrmodel.transform(testDF)\n",
    "\n",
    "predictonDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97c65958-2ab8-40a6-a6cc-dfcfdbf71af4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "|label|rawPrediction                         |probability                              |prediction|\n",
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "|0.0  |[4.026156743176436,-4.026156743176436]|[0.9824700109051254,0.017529989094874576]|0.0       |\n",
      "+-----+--------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictonDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9fd081c0-c55b-4899-b250-d3167dec64c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3481b64-857b-41c0-86ec-4a037f2c0462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.98134882220071\n",
      "Area under the ROC curve =  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import BinaryClassificationEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Build the BinaryClassificationEvaluator object 'evaluator'\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Calculate the accracy and print its value\n",
    "accuracy = predictonDF.filter(predictonDF.label == predictonDF.prediction).count()/float(predictonDF.count())\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "# evaluate(predictiondataframe) gets area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(predictonDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0fa6665c-852f-4ea5-885c-27f3e7551194",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The accuracy of our model is 98.13 % and the area under the ROC curve is 0.5. You can also find these metrics using the model summary as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71f00cbf-d4ac-4528-b6c5-c2d64bb82992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9824700109051254\n",
      "Area under the ROC curve =  0.5\n",
      "Precision =  0.9652473223279173\n",
      "Recall =  0.9824700109051254\n",
      "F1 Score =  0.973782520813235\n"
     ]
    }
   ],
   "source": [
    "# Create model summary object\n",
    "lrmodelSummary = lrmodel.summary\n",
    "\n",
    "# Print the following metrics one by one: \n",
    "# 1. Accuracy\n",
    "# Accuracy is a model summary parameter\n",
    "print(\"Accuracy = \", lrmodelSummary.accuracy)\n",
    "# 2. Area under the ROC curve\n",
    "# Area under the ROC curve is a model summary parameter\n",
    "print(\"Area under the ROC curve = \", lrmodelSummary.areaUnderROC)\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "# Precision is a model summary parameter\n",
    "print(\"Precision = \", lrmodelSummary.weightedPrecision)\n",
    "# 4. Recall (True Positive Rate)\n",
    "# Recall is a model summary parameter\n",
    "print(\"Recall = \", lrmodelSummary.weightedRecall)\n",
    "# 5. F1 Score (F-measure)\n",
    "# F1 Score is a model summary method\n",
    "print(\"F1 Score = \", lrmodelSummary.weightedFMeasure())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "923eb5f9-3e99-4ae1-b851-53530bab68f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f0501454-5dfc-4a43-ba2b-217ab9ab2fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set =  36680\n",
      "Observations in testing set =  15495\n"
     ]
    }
   ],
   "source": [
    "# We spilt the data into 70-30 set\n",
    "# Training Set - 70% obesevations\n",
    "# Testing Set - 30% observations\n",
    "trainDF, testDF =  vecindexedDF.randomSplit([0.7,0.3], seed = 2020)\n",
    "\n",
    "# print the count of observations in each set\n",
    "print(\"Observations in training set = \", trainDF.count())\n",
    "print(\"Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6441e3eb-d105-405d-a380-956e0f08f6b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------------------------------------+----------+\n",
      "|label|rawPrediction|probability                              |prediction|\n",
      "+-----+-------------+-----------------------------------------+----------+\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "|0.0  |[9916.0,4.0] |[0.9995967741935484,4.032258064516129E-4]|0.0       |\n",
      "+-----+-------------+-----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the DecisionTree function from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Build the DecisionTree object 'dt' by setting the required parameters\n",
    "# We will pass the VectorIndexed columns as featureCol for Decision Tree. Since they can handle categorical indexes\n",
    "dt = DecisionTreeClassifier(featuresCol=\"indexed_features\", labelCol=\"label\",maxDepth= 10)\n",
    "\n",
    "# fit the DecisionTree object on the training data\n",
    "dtmodel = dt.fit(trainDF)\n",
    "\n",
    "#This DecisionTreeModel can be used as a transformer to perform prediction on the testing data\n",
    "dtpredictonDF = dtmodel.transform(testDF)\n",
    "\n",
    "dtpredictonDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0be2abfe-1c68-48b4-83d5-f42b46078de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "826b6bf9-6637-42cf-8514-ffd345c1831d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 100:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9780574378831881\n",
      "Area under the ROC curve =  0.5421588500623729\n"
     ]
    }
   ],
   "source": [
    "# import BinaryClassificationEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Build the BinaryClassificationEvaluator object 'evaluator'\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Calculate the accracy and print its value\n",
    "accuracy = dtpredictonDF.filter(dtpredictonDF.label == dtpredictonDF.prediction).count()/float(dtpredictonDF.count())\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "# evaluate(predictiondataframe) gets area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(dtpredictonDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b5b4c56-9e15-4225-b663-18d9559ac169",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.970888654696996\n",
      "Area under the ROC curve =  0.5421588500623729\n",
      "Precision =  0.970888654696996\n",
      "Recall =  0.970888654696996\n",
      "F1 Score =  0.970888654696996\n"
     ]
    }
   ],
   "source": [
    "# import MulticlassClassificationEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Build the MulticlassClassificationEvaluator object 'evaluator'\n",
    "multievaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(dtpredictonDF, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(dtpredictonDF))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(dtpredictonDF, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(dtpredictonDF, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(dtpredictonDF, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63021434-27a5-4b4f-95d2-209f29a5e488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e24e203f-2521-4242-9feb-580d4feb6d1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 136:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------+----------------------------------------+----------+\n",
      "|label|rawPrediction                          |probability                             |prediction|\n",
      "+-----+---------------------------------------+----------------------------------------+----------+\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "|0.0  |[19.704623707682273,0.2953762923177289]|[0.9852311853841135,0.01476881461588644]|0.0       |\n",
      "+-----+---------------------------------------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the RandomForestClassifier function from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Build the RandomForestClassifier object 'dt' by setting the required parameters\n",
    "# We will pass the VectorIndexed columns as featureCol for Random Forest. Since they can handle categorical indexes\n",
    "rf = RandomForestClassifier(featuresCol=\"indexed_features\", labelCol=\"label\")\n",
    "\n",
    "# fit the RandomForestClassifier object on the training data\n",
    "rfmodel = rf.fit(trainDF)\n",
    "\n",
    "#This RandomForestClassifierModel can be used as a transformer to perform prediction on the testing data\n",
    "rfpredictonDF = rfmodel.transform(testDF)\n",
    "\n",
    "rfpredictonDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ec93f86-fd8f-438e-8f6f-8a461e618e38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bbcc7b2e-99e3-4314-bc8c-5d2b8c38d26e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9721110185586135\n",
      "Area under the ROC curve =  0.8166641787274829\n",
      "Precision =  0.9721110185586135\n",
      "Recall =  0.9721110185586135\n",
      "F1 Score =  0.9721110185586135\n"
     ]
    }
   ],
   "source": [
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(rfpredictonDF))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5ee253c-6247-4b2b-b3eb-014d6e417e17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b0507e7-0175-4b75-9f46-f3298b729b7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will split the stroke df into train and test split and pass the training set to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b94c37d3-66a3-4394-b850-c56e787643e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set =  36680\n",
      "Observations in testing set =  15495\n"
     ]
    }
   ],
   "source": [
    "# We spilt the data into 70-30 set\n",
    "# Training Set - 70% obesevations\n",
    "# Testing Set - 30% observations\n",
    "trainDF, testDF =  strokeDF.randomSplit([0.7,0.3], seed = 2020)\n",
    "\n",
    "# print the count of observations in each set\n",
    "print(\"Observations in training set = \", trainDF.count())\n",
    "print(\"Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc384de7-826a-4872-a87d-638a287592c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Pipeline from pyspark.ml package\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Build the pipeline object by providing stages(transformers + Estimator) \n",
    "# that you need the dataframe to pass through\n",
    "# Transfoermers - binarizer, bucketizer, indexers, encoder, assembler\n",
    "# Estimator - lr\n",
    "lrpipeline = Pipeline(stages=[binarizer, bucketizer, indexers, encoder, assembler, lr])\n",
    "\n",
    "# fit the pipeline for the trainind data\n",
    "lrpipelinemodel = lrpipeline.fit(trainDF)\n",
    "\n",
    "# transform the data\n",
    "lrpipelinepredicted = lrpipelinemodel.transform(testDF)\n",
    "\n",
    "# view some of the columns generated\n",
    "lrpipelinepredicted.select('label', 'rawPrediction', 'probability', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e647501-5f78-408e-97bb-203604347a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9721110185586135\n",
      "Area under the ROC curve =  0.5\n",
      "Precision =  0.9721110185586135\n",
      "Recall =  0.9721110185586135\n",
      "F1 Score =  0.9721110185586135\n"
     ]
    }
   ],
   "source": [
    "# Build the MulticlassClassificationEvaluator object 'evaluator'\n",
    "multievaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(lrpipelinepredicted))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(lrpipelinepredicted, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffa661ab-984d-489f-9c9d-4f89ccf9b67c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Persistence\n",
    "Model persistence means saving your model to a disk. After you finalize your model for prediction depending upon the performance, you need to save the model to the disk. Let's say, you finalize 'lrpipelinemodel' to be used for in production environment i.e. in your application. We use the following code to save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f583b5c9-c1d2-418f-a9ce-4199ac78abce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Saving pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5fdbc2e5-cc2e-4499-885d-f7f644fc529d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use save() method to save the model\n",
    "# write().overwrite() is usually used when you want to replace the older model with a new one\n",
    "# It might happen that you wish to retrain your model and save it at the same the place\n",
    "lrpipelinemodel.write().overwrite().save(\"lrmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73c21a7b-08d9-40aa-aac1-a004a7910265",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Loading pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce103e9d-424d-418c-aea1-0b0b222e65cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "|  0.0|[4.02615674317643...|[0.98247001090512...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import PipelineModel from pyspark.ml package\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# load the model from the location it is stored\n",
    "# The loaded model acts as PipelineModel\n",
    "pipemodel = PipelineModel.load(\"lrmodel\")\n",
    "\n",
    "# use the PipelineModel object to perform prediciton on test data. \n",
    "# Use .transform() to perfrom prediction\n",
    "prediction = pipemodel.transform(testDF)\n",
    "\n",
    "# print the results\n",
    "prediction.select('label', 'rawPrediction', 'probability', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a31404f-9ad1-44a4-a4b6-92ee1f500cf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Faculty Notebook",
   "notebookOrigID": 2316234479472288,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
